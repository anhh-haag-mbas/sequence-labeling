{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Autoreload allows for automatic reloading of imported modules\n",
    "%load_ext autoreload\n",
    "# Load main using autoreload\n",
    "%aimport main\n",
    "# Automatically reload all things imported using autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_lookup/Identity:0\", shape=(10,), dtype=float32)\n",
      "[<tf.Variable 'embedding/embedding:0' shape=(10, 10) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "emb = Embedding(10,10)\n",
    "emb.build(None)\n",
    "print(emb.call(10))\n",
    "print(emb.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/martoko/Downloads/ud/ud-treebanks-v2.3/UD_German-PUD/de_pud-ud-test.conllu\"\n",
    "sentences = main.Sentences(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SENTENCE LENGTH ###\n",
      "56\n",
      "### SAMPLE WORDS ###\n",
      "['Osborne', 'meldete', 'sich', 'bei', 'einer']\n",
      "### SAMPLE TAGS ###\n",
      "['PROPN', 'VERB', 'PRON', 'ADP', 'DET']\n"
     ]
    }
   ],
   "source": [
    "print(\"### SENTENCE LENGTH ###\")\n",
    "print(sentences.sentence_length)\n",
    "print(\"### SAMPLE WORDS ###\")\n",
    "print(sentences.words()[25][0:5])\n",
    "print(\"### SAMPLE TAGS ###\")\n",
    "print(sentences.tags()[25][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### PADDED SAMPLE WORDS ###\n",
      "[['„', 'Ei', 'Gr', 'de', 'di', 'Üb', 'is', 'fü', 'di', 'Ve', 'St', 'ne', ',', 'ei', 'fr', 'Ma', 'hi', 'ni', '“', ',', 'sc', 'Ob', 'So', 'Ko', 'Sc', 'am', 'an', 'de', 'Mo', 'in', 'ei', 'Bl', '.', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__'], ['Fü', 'al', ',', 'di', 'So', '-', 'Me', '-', 'Üb', 'au', 'de', 'Ca', 'Hi', 've', ',', 'wi', 'di', 'Üb', 'ei', 'we', 'an', 'se', '.', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__']]\n",
      "### PADDED SAMPLE TAGS ###\n",
      "[['PU', 'DE', 'NO', 'DE', 'AD', 'NO', 'AU', 'AD', 'DE', 'AD', 'NO', 'AD', 'PU', 'DE', 'AD', 'NO', 'AD', 'AD', 'PU', 'PU', 'VE', 'PR', 'NO', 'PR', 'PR', '_', 'AD', 'DE', 'NO', 'AD', 'DE', 'NO', 'PU', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__'], ['AD', 'NO', 'PU', 'PR', 'NO', 'PU', 'NO', 'PU', 'NO', 'AD', 'DE', 'NO', 'NO', 'VE', 'PU', 'AU', 'DE', 'NO', 'DE', 'NO', 'AD', 'AU', 'PU', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__', '__']]\n"
     ]
    }
   ],
   "source": [
    "print(\"### PADDED SAMPLE WORDS ###\")\n",
    "print([[word[0:2] for word in group] for group in sentences.padded_words()[0:2]])\n",
    "print(\"### PADDED SAMPLE TAGS ###\")\n",
    "print([[tag[0:2] for tag in group] for group in sentences.padded_tags()[0:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TRAINING DATA LENGTH ###\n",
      "900 900\n",
      "\n",
      "### VALIDATION DATA LENGTH ###\n",
      "50 50\n",
      "\n",
      "### TEST DATA LENGTH ###\n",
      "50 50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"### TRAINING DATA LENGTH ###\")\n",
    "print(len(sentences.training_word_ids), len(sentences.training_tag_ids))\n",
    "print()\n",
    "\n",
    "print(\"### VALIDATION DATA LENGTH ###\")\n",
    "print(len(sentences.validation_word_ids), len(sentences.validation_tag_ids))\n",
    "print()\n",
    "\n",
    "print(\"### TEST DATA LENGTH ###\")\n",
    "print(len(sentences.test_word_ids), len(sentences.test_tag_ids))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SAMPLE WORDS ###\n",
      "['Ein', 'Element', ',', 'das', 'diese']\n",
      "[2, 1216, 13, 83, 175]\n",
      "\n",
      "### SAMPLE TAGS ###\n",
      "['DET', 'NOUN', 'PUNCT', 'PRON', 'DET']\n",
      "[2, 3, 1, 12, 2]\n",
      "\n",
      "### WORD DICTIONARY SAMPLE ###\n",
      "[('__unk__', 0), ('„', 1), ('Ein', 2), ('Großteil', 3), ('des', 4)]\n",
      "6185 words\n",
      "\n",
      "### TAG DICTIONARY SAMPLE ###\n",
      "[('__unk__', 0), ('PUNCT', 1), ('DET', 2), ('NOUN', 3), ('ADJ', 4)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"### SAMPLE WORDS ###\")\n",
    "print(sentences.test_words()[3][0:5])\n",
    "print(sentences.test_word_ids[3][0:5])\n",
    "print()\n",
    "\n",
    "print(\"### SAMPLE TAGS ###\")\n",
    "print(sentences.test_tags()[3][0:5])\n",
    "print(sentences.test_tag_ids[3][0:5])\n",
    "print()\n",
    "\n",
    "print(\"### WORD DICTIONARY SAMPLE ###\")\n",
    "print(list(sentences.id_by_word.items())[0:5])\n",
    "print(sentences.word_count, \"words\")\n",
    "print()\n",
    "\n",
    "print(\"### TAG DICTIONARY SAMPLE ###\")\n",
    "print(list(sentences.id_by_tag.items())[0:5])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### SHAPE ###\n",
      "Shape: (ENTRIES, DIMENSIONS)\n",
      "Shape: (7, 5)\n",
      "\n",
      "### EMBEDDING EXAMPLE ###\n",
      "[[-0.33783126 -0.3548144  -0.38183865 -0.53107405  0.41663224]\n",
      " [-0.33783126 -0.3548144  -0.38183865 -0.53107405  0.41663224]\n",
      " [-0.33783126 -0.3548144  -0.38183865 -0.53107405  0.41663224]\n",
      " [-0.01512766 -0.4023714  -0.66070426 -0.60552883 -0.19932711]\n",
      " [-0.5685188   0.42804068  0.6346535  -0.13262516 -0.5055536 ]\n",
      " [-0.02324939 -0.70452875 -0.16063023 -0.39289427 -0.2674307 ]\n",
      " [-0.25600165 -0.5676973  -0.3175895  -0.5494278   0.46528214]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import Session\n",
    "\n",
    "embedding_dimensions = 5\n",
    "input = [1,1,1,2,3,4,5]\n",
    "embedding = Embedding(len(input), embedding_dimensions)(input)\n",
    "\n",
    "model = embedding\n",
    "print(\"### SHAPE ###\")\n",
    "print(\"Shape: (ENTRIES, DIMENSIONS)\")\n",
    "print(\"Shape:\", model.shape)\n",
    "print()\n",
    "\n",
    "session = Session()\n",
    "# Initialize unitialized variables in layers\n",
    "# In this case just the embedding layer\n",
    "print(\"### EMBEDDING EXAMPLE ###\")\n",
    "session.run(tf.global_variables_initializer())\n",
    "print(session.run(model))\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### EMBEDDING SHAPE ###\n",
      "Embedding shape: (SENTENCES, WORDS, DIMENSIONS)\n",
      "Embedding shape: (900, 56, 16)\n",
      "\n",
      "### DENSE SHAPE ###\n",
      "Dense shape: (SENTENCES, WORDS, DIMENSIONS)\n",
      "Dense shape: (900, 56, 19)\n",
      "\n",
      "### RUN ###\n",
      "[[[-5.09209372e-03  1.50791300e-03 -4.93746251e-03 ... -1.89534873e-02\n",
      "    1.36761302e-02 -4.72974638e-03]\n",
      "  [ 9.46029276e-03  5.66457910e-03 -5.51265851e-03 ... -3.83946998e-03\n",
      "   -1.07055688e-02 -2.00245306e-02]\n",
      "  [ 8.24543461e-03 -2.45409319e-03 -2.57360619e-02 ...  1.64704807e-02\n",
      "    3.19728479e-02  2.35756561e-02]\n",
      "  ...\n",
      "  [-6.99469727e-03 -1.94459073e-02 -1.61688332e-03 ... -3.60888150e-03\n",
      "   -1.81634948e-02 -9.54899099e-03]\n",
      "  [-6.99469727e-03 -1.94459073e-02 -1.61688332e-03 ... -3.60888150e-03\n",
      "   -1.81634948e-02 -9.54899099e-03]\n",
      "  [-6.99469727e-03 -1.94459073e-02 -1.61688332e-03 ... -3.60888150e-03\n",
      "   -1.81634948e-02 -9.54899099e-03]]\n",
      "\n",
      " [[-2.66476944e-02  1.64068770e-02 -1.69292446e-02 ... -6.20297249e-03\n",
      "   -7.58355483e-04  8.49461742e-03]\n",
      "  [ 1.61188329e-03  5.35426149e-03 -1.23227732e-02 ... -1.90645177e-02\n",
      "   -1.61159914e-02 -1.30523862e-02]\n",
      "  [-2.03398541e-02  3.36966291e-03 -1.47594633e-02 ... -2.04828046e-02\n",
      "   -1.40545405e-02  1.52174700e-02]\n",
      "  ...\n",
      "  [-6.99469727e-03 -1.94459073e-02 -1.61688332e-03 ... -3.60888150e-03\n",
      "   -1.81634948e-02 -9.54899099e-03]\n",
      "  [-6.99469727e-03 -1.94459073e-02 -1.61688332e-03 ... -3.60888150e-03\n",
      "   -1.81634948e-02 -9.54899099e-03]\n",
      "  [-6.99469727e-03 -1.94459073e-02 -1.61688332e-03 ... -3.60888150e-03\n",
      "   -1.81634948e-02 -9.54899099e-03]]\n",
      "\n",
      " [[-9.55339428e-03  2.28673536e-02  9.66811832e-03 ... -1.19785732e-02\n",
      "   -2.59379055e-02  1.39309224e-02]\n",
      "  [ 1.08368928e-02 -3.61595266e-02 -1.94924064e-02 ...  3.82526442e-02\n",
      "    1.47736296e-02  2.07600184e-03]\n",
      "  [-2.86263786e-02  3.00695980e-03  7.86243193e-03 ... -1.27824303e-03\n",
      "   -1.23525597e-02  4.83262986e-02]\n",
      "  ...\n",
      "  [-6.99469727e-03 -1.94459073e-02 -1.61688332e-03 ... -3.60888150e-03\n",
      "   -1.81634948e-02 -9.54899099e-03]\n",
      "  [-6.99469727e-03 -1.94459073e-02 -1.61688332e-03 ... -3.60888150e-03\n",
      "   -1.81634948e-02 -9.54899099e-03]\n",
      "  [-6.99469727e-03 -1.94459073e-02 -1.61688332e-03 ... -3.60888150e-03\n",
      "   -1.81634948e-02 -9.54899099e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.29166599e-02  1.31313242e-02  2.96873450e-02 ... -1.01383850e-02\n",
      "   -1.32803852e-03  5.50732389e-03]\n",
      "  [ 1.59121305e-02 -1.43260239e-02 -1.20517919e-02 ...  1.36062074e-02\n",
      "    2.20848732e-02  7.22743617e-03]\n",
      "  [ 3.76050211e-02 -2.10152306e-02  2.64686067e-04 ... -9.95592587e-03\n",
      "   -4.82394127e-03 -1.34202717e-02]\n",
      "  ...\n",
      "  [-6.99469727e-03 -1.94459073e-02 -1.61688332e-03 ... -3.60888150e-03\n",
      "   -1.81634948e-02 -9.54899099e-03]\n",
      "  [-6.99469727e-03 -1.94459073e-02 -1.61688332e-03 ... -3.60888150e-03\n",
      "   -1.81634948e-02 -9.54899099e-03]\n",
      "  [-6.99469727e-03 -1.94459073e-02 -1.61688332e-03 ... -3.60888150e-03\n",
      "   -1.81634948e-02 -9.54899099e-03]]\n",
      "\n",
      " [[ 4.35139053e-03 -2.40012389e-02  3.13514820e-03 ...  1.31931314e-02\n",
      "    1.32148117e-02  5.77206258e-04]\n",
      "  [-1.53062185e-02 -2.46404186e-02  9.50490683e-03 ...  6.99741580e-03\n",
      "   -7.25779682e-05  6.08985173e-03]\n",
      "  [-1.13166068e-02 -2.10536476e-02 -5.68325212e-03 ...  2.01425143e-02\n",
      "    2.10221559e-02 -1.07469736e-02]\n",
      "  ...\n",
      "  [-6.99469727e-03 -1.94459073e-02 -1.61688332e-03 ... -3.60888150e-03\n",
      "   -1.81634948e-02 -9.54899099e-03]\n",
      "  [-6.99469727e-03 -1.94459073e-02 -1.61688332e-03 ... -3.60888150e-03\n",
      "   -1.81634948e-02 -9.54899099e-03]\n",
      "  [-6.99469727e-03 -1.94459073e-02 -1.61688332e-03 ... -3.60888150e-03\n",
      "   -1.81634948e-02 -9.54899099e-03]]\n",
      "\n",
      " [[ 1.89731047e-02 -2.80298889e-02  1.78785324e-02 ...  4.42067832e-02\n",
      "   -1.39825288e-02  2.38555484e-02]\n",
      "  [-1.53022585e-03 -3.16872820e-03 -8.62275995e-03 ...  9.19584278e-03\n",
      "    1.97270960e-02 -1.59540903e-02]\n",
      "  [-1.42351016e-02  1.69252828e-02 -8.48381408e-03 ... -8.24043714e-03\n",
      "   -3.89146293e-03 -3.59362923e-03]\n",
      "  ...\n",
      "  [-6.99469727e-03 -1.94459073e-02 -1.61688332e-03 ... -3.60888150e-03\n",
      "   -1.81634948e-02 -9.54899099e-03]\n",
      "  [-6.99469727e-03 -1.94459073e-02 -1.61688332e-03 ... -3.60888150e-03\n",
      "   -1.81634948e-02 -9.54899099e-03]\n",
      "  [-6.99469727e-03 -1.94459073e-02 -1.61688332e-03 ... -3.60888150e-03\n",
      "   -1.81634948e-02 -9.54899099e-03]]]\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "# Our input to the model\n",
    "word_ids = sentences.training_word_ids\n",
    "sentence_length = sentences.sentence_length\n",
    "\n",
    "# The first part of the model translates ids to vectors\n",
    "embedding_dimensions = 16\n",
    "embedding = Embedding(sentences.word_count, embedding_dimensions)(word_ids)\n",
    "print(\"### EMBEDDING SHAPE ###\")\n",
    "print(\"Embedding shape: (SENTENCES, WORDS, DIMENSIONS)\")\n",
    "print(\"Embedding shape:\", embedding.shape)\n",
    "print()\n",
    "\n",
    "# The second part is just a bunch of dense layers\n",
    "dense = Dense(sentences.tag_count)(embedding)\n",
    "print(\"### DENSE SHAPE ###\")\n",
    "print(\"Dense shape: (SENTENCES, WORDS, DIMENSIONS)\")\n",
    "print(\"Dense shape:\", dense.shape)\n",
    "print()\n",
    "\n",
    "session = Session()\n",
    "# Initialize unitialized variables in layers\n",
    "# In this case just the embedding layer\n",
    "print(\"### RUN ###\")\n",
    "session.run(tf.global_variables_initializer())\n",
    "print(session.run(dense))\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = emb\n",
    "# lstm = Lstm(num_units=10)\n",
    "# lstm.call(id_by_word, lstm.zero_state(3, dtype=tf.float32))\n",
    "# lstm.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
