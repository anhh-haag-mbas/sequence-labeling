

\section{Conclusion}

POS findings:

\begin{itemize}
    \item Our TensorFlow implementation of CRF is buggy
    \item The single layer implementations of \texttt{Bi-LSTM} requires
        significantly more epochs to converge during trainer than the 2 layer
        model when the batch size increase
    \item CRF makes the models converge faster during training
    \item CRF improves the accuracy on the single layer \texttt{Bi-LSTM}
        implementation of PyTorch, but has negligible effect on the DyNet
        implementation
    \item PyTorch and DyNet performs very similar for \texttt{Bi-LSTM-CRF}
\end{itemize}

NER findings:

\begin{itemize}
    \item CRF makes the models converge faster during training
    \item Our models perform poorly despite fast convergence, probably a result
      of using accuracy instead of $F_1$ for early stopping.
    \item Adding CRF to the models seems to help, which is in line with our
      expectations.
    \item We identified several problems with the Wikipedia extracted NER data.
      Some languages were missing or almost missing some tags.
      The Japanese data was split per character and not per word.
\end{itemize}

\pagebreak
