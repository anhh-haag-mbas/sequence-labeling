

\section{Conclusion}

Our experiments have yielded widely different results for the different
configurations.

% # factual things #
The \texttt{Bi-LSTM-CRF} model seems to generally converge faster than the
\texttt{Bi-LSTM} models. Single layer implementations of \texttt{Bi-LSTM}
seem to converge much slower than the 2 layer implementations, as batch size
increases.

Adding CRF seems to improve the accuracy of the NER task as expected. CRF seems
to have a very little impact on our DyNet implementation compared to our PyTorch
implementation. On the POS task, CRF consistently improved the PyTorch
implementation for larger batch size, while the DyNet implementation generally
did best without CRF.\ For TensorFlow, CRF hurt the accuracy to a degree that
makes us believe, that there is some error in the implementation.

Also, our TensorFlow implementation did not mask out padding, which caused a
significant drop in accuracy, as such our results mostly apply to PyTorch and
DyNet.

Our implementations perform quite poorly on the NER task, despite converging
quite quickly. This is likely a result of doing early stopping on accuracy
instead of $F_1$ score. On the POS task however, we were able to consistently
get accuracies over 90\% for our best models.

Throughout our results, we find that the 2 layered Bi-LSTM implementation in
DyNet had the best performance of all our implementations. The
\texttt{Bi-LSTM-CRF} model of PyTorch had the second best results, similar to
(albeit a bit lower) the DyNet implementations.

Another general tendency in the results, is that increasing batch sizes hurt
performance. There are some exceptions, but overall this seem to be the case.

For Japanese, we observed a worse performance than for the other languages, both
in the POS and in the NER task. We found that, for the POS case, this can
probably be attributed to the fact, that Polyglot embedding for Japanese is
mostly character based, whereas the POS data exposed multi-character words. For
this reason, Japanese had a much higher OOV percentage than the other languages.

The tag distribution on the language data seems to play a part in the resulting
scores. For the NER data, we see that the languages with the most uneven tag
distribution or an exceptionally high number of non-entities performs more
irregularly. Also, we anticipate that we could have had better and more
comparable results, had the training, testing and validation data been more
aligned in terms of tag distribution.

In working with the frameworks, we experienced less inconvenience in working
with the two dynamic frameworks (DyNet and PyTorch) compared to the static one
(TensorFlow). Though TensorFlow has a richer community and a larger set of
online ressources, the level of abstraction was less suited for machine learning
beginners than that of DyNet and Pytorch were.

\pagebreak
