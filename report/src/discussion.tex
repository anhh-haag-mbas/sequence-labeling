
\section{Discussion}

\subsection{Possible sources of Discrepancies}

Throughout the report we have noted various problems and possible causes. To
give an overview and make it clear how to improve the validity of the results we
got, have we compiled a list of the issues.

\textbf{Data}

Our datasets for NER were not great 
Make sure the datasets are similar and that the testing, validation, and
training sets are representative of each other. 

Inspect the data to avoid datasets with glaring errors such as very low number
of distinct tokens, badly balanced distribution of tags, or datasets such as the
Japanese NER data where the tokens are split in unnatural ways.

If using the same NER dataset as we did in this project. There is no need to
limit the size of the sets to be similar to POS unless a comparison between the
tasks is made. Since we didn't compare the tasks we could have used more data
for NER which may have mitigated some of the issues we encountered. Also make
sure to shuffle the sentences before splitting the data. A deterministic
shuffling method can be used for reproducability.

\textbf{Models}

Our models were different on multiple fronts.
DyNet used 2 layer Bi-LSTM.
The loss for batching was mean vs sum.
CRF implementations weren't aligned.
We implemented early stopping differently between DyNet and PyTorch.

We encountered a problem for the word embedding where a lot of the tokens were
out of vocabulary.
Make sure the embedding works on the data. Either the data is bad, or you need a
task specific embedding for your problem.

Be sure to agree on each parameter of the model to avoid situations where 

We trained our models on accuracy, which isn't necessarily helpful when testing
on F1. 
Train on the thing you evaluate on.


\textbf{Results}

We didn't count correctly tagged entities for the NER task.
We didn't log the loss of the trainer or the validation.




Different ways of splitting japanese (and others?) data, CoNNL-U seem to split
based on some ``smart'' method, whereas our BIO data is just single character
each. The polyglot embedding for japanese though might also be single character.

Differences in implementations of batching between framework. Autobatching
Mean vs sum of losses.

Takes different number of epochs to converge, some actually gets 50 epochs.

Differences in implementation of CRF.\

The issue in PyTorch where not every sentence is used doesn't affect the
training since 1, 8, and 32 are whole dividers of the 4000 training sentences.

Different way to count number of epochs run

Without CRF we see that 2-layer Bi-LSTM performs better. With CRF the difference is negligible.
Especially as the batch size increases.

As expected, for a single epoch and 32 batch size, the models perform worse, 
but we see that dynet performs significantly worse than pytorch. This may be 
because of the 2-Layer Bi-LSTM in DyNet which requires more training before 
performing reasonably. It can also be explained by the issue where pytorch
doesn't pick the best model but rather the latest one.

Batch size have less influence on the performance given more training.
With higher batch size there are fewer updates (backpropogations) per epcoh,
this may also be interesting to research.

DyNet doesn't train as much when adding a CRF layer, possibly because of a bad
implementation or integration. Tensorflow similarly changes behaviour when using
CRF, but it can't stop training.

\subsection{Frameworks evaluation}\label{subsec:frameworks}

Pros and cons of each framework.

\subsubsection{Dynet}

The performance gain when running on batches may be because of fewer
backpropogations rather than any form of parallelization.

DyNet performs significantly slower when run with a CRF layer, this may suggest
that the implementation used isn't optimized. 


% nvprof for profilling

\textbf{The learning proccess}

My general approach to learning dynet was to ease myself into it, following
simple tutorials like an XOR example, and building from there. With an
understanding of how a simple network works I could try to create my own on a
toy problem and see if I really understood how stuff worked. One of the nice
things about Dynet is how close it feels to creating computational graphs.
Creating parameters and linking them together by applying mathematical
operations on them, made it easy to reason about the behavior of the model.
After the small examples it was simple to learn how to work an LSTM layer into
the model. LSTM in dynet hides a lot of the complexity associated with the
computations, but the interface was still intuitive to use. Get the initial
state, compute the next state based on input, get the output for the next layer
and save the state for the next part of the sequence. Working out how to add a
CRF layer on top was mostly a matter of understanding CRF rather than Dynet.
Figuring out that what we needed for CRF was simply a transformation matrix,
made it obvious that this matrix should just be added as a a parameter similar
to other parts of a model. From there I could take inspiration from a complete
model implemented in dynet and reuse their code for the CRF.\

\subsection{PyTorch}

\subsubsection*{The learning process}

PyTorch comes with a variety of official tutorials that gives introductions to
the fundamentals of PyTorch aswell as to how to use it in typical machine
learning scenarios (such as computer vision and NLP). These are of varying
quality and generally expects certain pre-existing knowledge of machine learning
concepts and models. Also, since one of the prime features of PyTorch is the
\code{tensor} object which is basically an NumPy \code{ndarray} that tracks its
own computational graph, experience with and fluency in NumPy is, if not a
pre-requisite, then at least a major advantage (I had no such thing).

However, working with PyTorch felt very smooth as the API provides most all
classes and functions needed to construct our models (and, obviously, a lot of
much more complex models) without the user having to worry about backpropagation
or any inner workings of the models. After a small time of getting used to the
documentation and the design pattern of the classes and functions, my experience
with PyTorch was that it was both intuitive and flexible to work with.

Vital to my learning process was the good number of unofficial tutorials and
walk-throughs, that has been created by the community on blogs and websites.
Especially the website Medium has several pedagogical articles, that aided my
understanding a lot (see~\cite{falcon2018lstms},~\cite{boulton2018conditional}
and~\cite{treviso2019crf} for examples).

\subsubsection*{A note on batching}

I described how I implemented batching in
Section~\ref{sec:setup-implementations-pytorch}, but I want to add a small point
about the API for mini-batches in PyTorch here.

Batching is rather poorly described by the documenation of the framework, and
there seem to be only sparse official description of how mini-batches are
expected to be arranged (~\cite{falcon2018lstms}).  As an example, working with
sequential data of variable length (such as sentences), each input element has
to padded so as to make the lengths uniform.  This may be common knowledge to an
experienced ML researcher, but neither the PyTorch documentation or official
tutorials (at least not under the topic of NLP) touch on this aspect.

Furthermore, to handle these batches of inputs of variable lengths, some modules
require the client to pass a masking tensor as argument (ie.\ a matrix of ones
and zeroes) to mask out padded values. Most ill-explained, however, is using
mini-batches together with the \code{LSTM} layer provided by the framework
itself. Here, it is necessary to pass the mini-batch of padded input sequences
(ordered by original sequence length in descending order) through
\code{pack\_padded\_sequence} function provided by the \code{torch.nn.utils.rnn}
module together with a list of the original lengths of each sequence. This
transforms the input so it can be handled properly by the \code{LSTM} layer, but
afterwards it has to be unpacked again using \code{pad\_packed\_sequence}. Not
only is this a rather intricate API, it is also only sparsely documented
considering how normal an operation training on mini-batches is.



\subsection{TensorFlow}

\subsubsection{Availability of information}

It is incredibly easy to find guides/blog posts/papers on how to use the Keras
interface for TensorFlow. It is furthermore incredibly easy to find code
examples and code for newer more experimental designs, such as CRF/LSTM,
implementations for TensorFlow/Keras.

Finding guides/help for TensorFlow, and NOT Keras, is quite difficult.

\subsubsection{High-level nature of Keras}

Because of the high level nature of Keras, a lot of implementation details are
hidden. This is fine if you are already perfectly familiar with the theory, but
makes it harder to understand what the framework is actually doing if you are
only somewhat familiar with the theory behind the different things  (LSTM, CRF,
MLP, etc.).

Contrast this with other frameworks or just with using TF directly, where
everything isn't conviently wrapped in high level `layers'. Using these you are
forced to understand what is actually going on, at least more than with Keras,
just to get any result.

Coupling the high level nature of Keras with the abundance of guides, means that
you could actually get started with working code within your given area, without
understanding any of the code or theory.

That is undesirable.

On the other hand if you are intimatley familiar with the theory working with TF
must be a breeze, as it abstracts all of the technical details away.

Working with non-standard `layers' is also quite easy, but does require a deeper
understanding of how TensorFlow works.

\subsubsection{Workflow}

I have worked a lot by finding examples on various blog posts and then copy
pasting them into a jupyter notebook.

I have then experimented with tweaking the examples and writing them together to
gain a deeper understanding of TensorFlow works. I would often come across
features of TF that I did not know existed, which would then prompt me to read
the TensorFlow docs on that feature.

This allowed me to quite quickly get working results, but it limited just how
deeply I got to understand the theory. That is because I relied on existing
implementations of bi-lstm/crf, and the way I got started was by looking at how
other people had already tackled the problems.

Had I developed the lstm and crf by hand, I would have been forced to gain a
much much deeper understanding of how they worked and of how TensorFlow worked.
This would however have required a much greater time dedication.

It was thus a weighting between time and deep understanding of
TensorFlow/Keras/CRF/LSTM.\


\subsection{Suggestions for further research}

Someone should really look into that urdu language. Also, whats up the the arabs
and the japanese??

\subsection{Improvements}

The models created in this project were kept simple to decrease unnecessary
complexity, as such a lot of changes can be made to improve their performance.
Here we present just a few of the changes that could be made that we are aware
of.

\subsubsection{Character embedding}

Similar to the way that pretrained word embeddings like Polyglot can be used to
significantly improve performance, including a character embedding, pretrained
or not, will also help the model make better predictions~\ref{misconceptions
yang zhang}.

Character embeddings, like word embeddings, work by mapping a character to a
vector representation. But character embeddings can contain a mapping from every
character to a vector representation since there are generally not a lot of
different characters in a language. Even languages like Chinese with thousands
of different characters is not an issue. This gives the model better predictive
power on words which are not part of the word embedding.

There are a lot of different ways to include a character embedding into a model.
The general approach is to map each character in a word to its own vector
representation and use an additional layer to read the sequence of character
representations and return a single vector. This is similar to the way we use an
LSTM to read the sequence of words in a sentence, but instead we only use a
single value. The most commonly used way to read the sequence of characters are
LSTM or CNN layers. They perform very similarly, but CNN are a lot faster to
train~\ref{misconceptions yang zhang}.

A character embedding can contain a mapping from not just a single character,
but combinations of two or more characters or so call n-grams. Since a lot of
languages use a relative small alphabet, a three character embedding could be
less than a million different values, which while big is still manageable.

The benefit of using a character embedding is that a model will always be able
to learn a unique representation for each word, whereas a word embedding can
only learn as many representations as the number of words it knows. An
additional benefit is that many words contain character level information which
a word embedding may not learn. As an example, words which are capitalized are
more likely to be named entities, than words which are uncapitalized, and words
which ends in ``ing'' are likely verbs.

\subsubsection{Stacked Bi-LSTM}

As has been mentioned before, the DyNet implementation uses a 2 layer Bi-LSTM.\
This is sometimes refered to as a stacked Bi-LSTM~\ref{Neural Network Methods
for Natural Language Proccessing ch. 14.5}. While not clear how exactly this
gives a better model accuracy, it has been shown empirically to help~\ref{Papers
using 2 layer}.

An explanation could simply be that adding additional Bi-LSTM layers increases
the complexity of the model which allows it to represent more complicated
functions. Similar to the way adding additional linear layers to a model can
improve a model. While stacked linear layers is in many cases equivalent to
increasing the dimensionality of the weights and biases~\ref{Deep learning 199,
Barron 1993} this may or may not be the case for Bi-LSTMs.

\subsubsection{DyNet manual batching}

For DyNet, we used autobatching to implement mini-batches. This is an algorithm
which based on the computational graph build, can create batches for optimized
training time. This shouldn't have any impact on the accuracy of the model, but
it improves the speed compared to running without. It is however still slower
than creating the batches manually and as such the DyNet implementation could be
improved in this sense if reducing the training time is important.


\subsection{Reflections}

\begin{itemize}
    \item Handling, analyzing and presenting large amounts of data was
        something, that we had no prior experience in. This was evident in the
        final phase of our project, where we struggled with the tools and
        techniques to properly work with the data we had created.
    \item Choosing to implement the same models in three different frameworks
        had more of an educational purpose than a scientific one. Due to
        complications in understanding the inner work of each of our one
        frameworks, the task of synchronizing the implementations became
        disproportionally difficult. Our results are therefore burdened by the
        uncertainty of whether our programs actually the work, as they are
        intended to.
\end{itemize}

\pagebreak
