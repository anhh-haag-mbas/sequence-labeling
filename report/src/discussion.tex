
\section{Discussion}

\subsection{Possible sources of error}

There might be and index error in my implementation of the Viterbi Algorithm.

Different ways of splitting japanese (and others?) data, CoNNL-U seem to split
based on some ``smart'' method, whereas our BIO data is just single character
each. The polyglot embedding for japanese though might also be single character.

\subsection{Frameworks evaluation}
\label{subsec:frameworks}

Pros and cons of each framework.

\subsubsection{Dynet}

The performance gain when running on batches may be because of fewer
backpropogations rather than any form of parallelization.

DyNet performs significantly slower when run with a CRF layer, this may suggest
that the implementation used isn't optimized. 


% nvprof for profilling

\textbf{The learning proccess}

My general approach to learning dynet was to ease myself into it, following
simple tutorials like an XOR example, and building from there. With an
understanding of how a simple network works I could try to create my own on a
toy problem and see if I really understood how stuff worked. One of the nice
things about Dynet is how close it feels to creating computational graphs.
Creating parameters and linking them together by applying mathematical
operations on them, made it easy to reason about the behavior of the model.
After the small examples it was simple to learn how to work an LSTM layer into
the model. LSTM in dynet hides a lot of the complexity associated with the
computations, but the interface was still intuitive to use. Get the initial
state, compute the next state based on input, get the output for the next layer
and save the state for the next part of the sequence. Working out how to add a
CRF layer on top was mostly a matter of understanding CRF rather than Dynet.
Figuring out that what we needed for CRF was simply a transformation matrix,
made it obvious that this matrix should just be added as a a parameter similar
to other parts of a model. From there I could take inspiration from a complete
model implemented in dynet and reuse their code for the CRF.


\subsection{Suggestions for further research}

Someone should really look into that urdu language. Also, whats up the the arabs
and the japanese??

\subsection{Improvements}

The models created in this project were kept simple to decrease unnecessary
complexity, as such a lot of changes can be made to improve their performance.
Here we present just a few of the changes that could be made that we are aware
of.

\subsubsection{Character embedding}

Similar to the way that pretrained word embeddings like Polyglot can be used to
significantly improve performance, including a character embedding, pretrained
or not, will also help the model make better predictions.

Character embeddings like word embeddings work by mapping a character to a
vector representation. Unlike word embeddings, can character embeddings contain
a mapping from every character to a vector representation. Usually a character
embedding contains a mapping from not just a single character, but combinations
of two or more characters or so call k-grams. Since a lot of languages use a
relative small alphabet, a three character embedding could contain  

Character embedding (either lstm or cnn), different optimizer perhaps, dynet manual batching as
opposed to auto batching, 2 layers bi-lstm, Auxiliary loss thingy magic

\subsection{Other stuff}

\begin{itemize}
    \item Handling, analyzing and presenting large amounts of data was
        something, that we had no prior experience in. This was evident in the
        final phase of our project, where we struggled with the tools and
        techniques to properly work with the data we had created.
    \item Choosing to implement the same models in three different frameworks
        had more of an educational purpose than a scientific one. Due to
        complications in understanding the inner work of each of our one
        frameworks, the task of synchronizing the implementations became
        disproportionally difficult. Our results are therefore burdened by the
        uncertainty of whether our programs actually the work, as they are
        intended to.
    \item It seems the Tensorflow implementation does not handle increasing
        batch sizes well. This is seen in Figure~\ref{chart:ner-acc-epo} and in
        Figure~\ref{chart:pos-acc-epo}. When training with early stopping using
        a patience of 3, Tensorflow looses accuracy for arabic, danish, hindu
        and urdu as the batch size increase from 1 to 8 and from 8 to 32. And
        except for japanese, a batch size of 1 yields the best accuracy on all
        other languages 
\end{itemize}

\pagebreak
