
\subsection{Named Entity Recognition}

This section will go into the details of the Named Entity Recognition task, what
the specifics are, considerations in regards to the data used, any issues we
came across, and the results of our experiments.

\subsubsection{Task definition}

Named Entity Recognition is a classification task of different types of
entities, such as places, people, organisation, times, and more. Entities can
consist of one or more words (and/or numbers), eg.\ ``Eurovision Song Contest
2019'' is a single entity of 3 words and a number. The classifier should
correctly identify the whole entity and not just part of it. We didn't take this
into consideration before running the experiments so our results are on a word
by word basis and doesn't tell us if the whole entity was correctly classified
or not.

The same entity might never show up in the dataset twice so it's important for
the classifier to be able to figure out based on context of the other words in
the sentence if there is an entity or not. It is impossible to simply remember
that a word refers to an entity since the word was most likely not seen during
training.

Usually the performance of a classifier for the NER task is measured by a $F_1$
Score, which considers the classifier's precision and recall. The precision is
the classifier's ability to correctly classify an entity given all attempts, and
recall is the ability to correctly classify an element given how many elements
exists. Say there are 100 entities in a document, the classifier finds 90
entities, 80 of them are correct, 10 of them are wrong, and 20 were not found.
From this we get the precision $P = {80 \over 80 + 10} = 0.88\ldots$, and recall
$R = {80 \over 80 + 20} = 0.8$. The $F_1$ score is simply the harmonic mean
between the precision and recall given by $F_1 = 2 \cdot {precision \cdot recall
\over precision + recall}$. Based on our example this would be $F_1 = 2 \cdot
{0.88 \ldots \cdot 0.8 \over 0.88\ldots + 0.8} = 0.8421$
~\ref{https://www.toyota-ti.ac.jp/Lab/Denshi/COIN/people/yutaka.sasaki/F-measure-YS-26Oct07.pdf}.

The rationale behind using the $F_1$ score is that usually a lot of the data is
non-entities. A classifier which classified nothing as an entity would have a
great accuracy on datasets with few entities. Since we didn't evaluate if whole
entities were classified correctly, we will base our score on the individual
words and the score might therefore be higher than otherwise. Since we aren't
comparing with models outside our own this shouldn't be an issue.

The reason the NER task is relevant is its usefulness in handling huge data
sizes, since the task is inherently about extracting information from text.
Extracting entities from a text would allow for easy tagging, comparisons
between documents, and linking to a knowledge base. A document mentioning
different locations and capital cities such as ``France'', ``Tokyo'', and ``New
Zealand'', could be about geografi or traveling. And similar a document
mentioning people associated with economy could be about that
\ref{https://www.aclweb.org/anthology/W03-0419}. 

The state of the art model for NER based off of
~\ref{https://paperswithcode.com/sota/named-entity-recognition-ner-on-conll-2003}
is ~\ref{https://arxiv.org/pdf/1903.07785v1.pdf}.

% TODO: Description of their model when I understand it.

They achieve an F1 score of 93.5 on the CoNLL 2003 English dataset, which is
0.41 points above the previous state of the art by
~\ref{https://www.aclweb.org/anthology/C18-1139}, the same model mentioned in
the previous section with state of the art results for POS.

\subsubsection{Data}\label{sec:experiments-ner-data}

For the NER task we used auto-annotated wikipedia data from~\ref{}{Cross-lingual
Name Tagging and Linking for 282 Languages}. As the title suggests, this dataset
contains annotated data for 282 languages in varying sizes. 

The datasets are all in the BIO (or IOB) format, which has three different
entities, organisations, people, and locations. The BIO format uses the
convention that the beginning of an entity is prefixes with a B and the
continuation of an entity is prefixed with an I (for inside). Tokens which are
not part of an entity is labeled O (for other). This gives the 7 labels, B-ORG,
B-PER, B-LOC, I-ORG, I-PER, I-LOC, and O. 

Since all the datasets were created based on wikipedia data we didn't have to
take varying sources into considerations, as was necessary for the POS task.

The data being auto-annotated means it might not be of the same standard as the
POS data. Ideally this shouldn't affect the experiments since the word orderings
are still the same, and systematic errors in the data would hopefully happen for
all languages in the datasets.

For Japanese which doesn't use spaces to differentiate between the beginning and
the end of words, the data is split and labeled character by character, this
differs from the POS data where words are split in a more natural way. This has
an influence on the results and means that Japanese isn't as comparable between
tasks as other languages. Since it turns out that our Polyglot embedding has a
somewhat similar way to split words it can have a positive effect on the
performance.

As the data used for training the model is the same data used for training the
polyglot word embeddings, the models don't have the same kind of variety from
the data as is the case for the POS task. This may result in worse performance
since the models might be more likely to overfit.

A table of the languages with the number of entities in their respective dataset
is given below in Table~\ref{}.

\subsubsection{Results and analysis}

Starting by looking at number of epochs trained
(Table~\ref{table:epochs-run-ner}), we see that all models terminated training
some time before hitting the upper bound, with the TensorFlow implementation of
\texttt{Bi-LSTM} with a batch size of 8 being the slowest to converge (36.8
epochs on average). Also, we see that \texttt{Bi-LSTM-CRF} models converged
faster for most every batch size and implementation.

\begin{table}[h!]
    \centering
    \begin{tabular}{l c c c|c c c}
        \toprule
        \multirow{2}{*}{\bfseries Batch size}     &
        \multicolumn{3}{c}{\bfseries Bi-LSTM}     &
        \multicolumn{3}{c}{\bfseries Bi-LSTM-CRF} \\
        \cmidrule(lr){2-7}
        & DyNet & PyTorch & TensorFlow
        & DyNet & PyTorch & TensorFlow \\
        \cmidrule(lr){1-7}
         1 &  9.49 & 13.43 & 21.00 &  8.14 &  7.66 &  6.43 \\
         8 &  9.86 & 22.43 & 36.80 &  9.97 & 11.46 &  8.69 \\
        32 & 12.57 & 29.06 & 31.51 & 11.57 & 14.63 & 10.91 \\
        \bottomrule
    \end{tabular}
    \caption{Average of epochs run across all seeds and languages when trained
        with early stopping.
        }\label{table:epochs-run-ner}
\end{table}

However, when turning our attention to the \textit{F1} scores
(Table~\ref{table:f1-total-ner}), the models are performing very poorly, which
does not fit with the expected results of a well converged model. We suspect the
main reason for this, is the fact that our models did the same \textit{accuracy}
test during training to decide when to stop as was done in the POS task. But
since we for the NER task want to optimize our \textit{F1} score, the accuracy
test is really not the optimal paramater to use in early stopping --- it would
of course had made more sense to test if the \textit{F1} score had stopped
improving.

% F1 SCORE
\begin{table}[h!]
    \centering
    \begin{tabular}{c l c c c|c c c}
        \toprule
        \multirow{2}{*}{\bfseries Language} &
        \multirow{2}{*}{\bfseries Batch size} &
        \multicolumn{3}{c}{\bfseries Bi-LSTM} &
        \multicolumn{3}{c}{\bfseries Bi-LSTM-CRF} \\
        \cmidrule(lr){3-8}
        && DyNet & PyTorch & Tensorflow & DyNet & PyTorch & Tensorflow \\

        \cmidrule(lr){1-8}
        \multirow{3}{*}{\bfseries ar}
        &  1 & 
        \underline{\textbf{83.5}} & 78.6 & 74.3 &
        82.6 & \underline{78.7} & \underline{75.0} \\
        &  8 & 
        \underline{\textbf{82.4}} & 62.5 & 47.4 &
        80.8 & \underline{77.3} & \underline{66.2} \\
        & 32 & 
        79.2 & 42.4 & 27.8 &
        \underline{\textbf{79.3}} & \underline{70.0} & \underline{48.9} \\

        \cmidrule(lr){1-8}
        \multirow{3}{*}{\bfseries da}
        &  1 &
        \underline{\textbf{77.0}} & 70.5 & \underline{54.0} &
        76.9 & \underline{71.8} & 49.9 \\
        &  8 &
        \underline{\textbf{74.9}} & 48.1 & 23.5 &
        74.6 & \underline{69.3} & \underline{35.1} \\
        & 32 &
        68.0 & 27.7 &  0.0 &
        \underline{\textbf{69.2}} & \underline{61.4} & \underline{20.8} \\

        \cmidrule(lr){1-8}
        \multirow{3}{*}{\bfseries hi}
        &  1 &
        \underline{\textbf{69.7}} & \underline{65.3} & 52.2 &
        69.4 & 62.8 & \underline{55.5} \\
        &  8 &
        \underline{\textbf{68.3}} & 55.1 & 26.2 &
        67.3 & \underline{63.4} & \underline{51.2} \\
        & 32 &
        \underline{\textbf{64.0}} & 33.6 &  1.2 &
        62.6 & \underline{56.9} & \underline{37.9} \\

        \cmidrule(lr){1-8}
        \multirow{3}{*}{\bfseries ja}
        &  1 &
        \underline{\textbf{58.1}} & \underline{34.9} & \underline{35.4} &
        52.6 &  8.1 & 14.8 \\
        &  8 &
        \underline{\textbf{54.3}} & \underline{24.9} & 13.6 &
        47.6 & 11.7 & \underline{47.3} \\
        & 32 &
        \underline{\textbf{47.9}} & 22.5 &  9.3 &
        44.5 & \underline{27.8} & \underline{45.2} \\

        \cmidrule(lr){1-8}
        \multirow{3}{*}{\bfseries no}
        &  1 &
        65.2 & \underline{49.7} & 29.9 &
        \underline{\textbf{67.1}} & 44.5 & \underline{47.1} \\
        &  8 &
        \underline{\textbf{63.2}} & 25.4 & \underline{11.8} &
        61.4 & \underline{48.1} &  3.4 \\
        & 32 &
        \underline{\textbf{56.0}} & 18.2 &  \underline{8.3 }&
        50.1 & \underline{37.2} &  2.2 \\

        \cmidrule(lr){1-8}
        \multirow{3}{*}{\bfseries ru}
        &  1 &
        \underline{\textbf{96.6}} & 95.3 & 81.2 &
        96.6 & \underline{95.8} & \underline{93.3} \\
        &  8 &
        96.2 & 93.5 & 68.7 &
        \underline{\textbf{96.4}} & \underline{95.9} & \underline{93.0} \\
        & 32 &
        95.5 & 83.5 & 43.3 &
        \underline{\textbf{95.7}} & \underline{95.2} & \underline{87.2} \\

        \cmidrule(lr){1-8}
        \multirow{3}{*}{\bfseries ur}
        &  1 &
        \underline{97.8} & 96.8 & 95.2 &
        97.4 & \underline{\textbf{98.0}} & \underline{97.6} \\
        &  8 &
        \underline{\textbf{97.8}} & 93.6 & 82.9 &
        97.6 & \underline{97.7} & \underline{96.9} \\
        & 32 &
        96.3 & 84.5 & 71.1 &
        \underline{\textbf{97.4}} & \underline{96.5} & \underline{96.3} \\
        \bottomrule
    \end{tabular}
    \caption{F1 scores for NER experiments by language and batch
        size. Bold: highest accuracy for batch size plus language. Underline:
        highest accuracy for framework between \texttt{Bi-LSTM} and
        \texttt{Bi-LSTM-CRF}.
    }\label{table:f1-total-ner}
\end{table}


Furthermore, the loss function should also have been utilized differently. The
motivation for using the \textit{F1} score, is to focus attention on the models
ability to classify entities and not reward it too much for identifying
`O'-tokens. But for our cross-entropy loss, a true prediction of a tag `O' has
had the same influence as the true prediction of an entity tag. Since there are
a lot more non-entities than entities, it is reasonable to suggest, that the
models will have tuned their parameters to correctly classify non-entities and
accepted a lower accuracy rate for entity tags. This is also supported by the
results shown in Figure~\ref{chart:tag-acc-ner}.

\begin{figure}[h!]
    \includegraphics[width=\textwidth]{tag-acc-ner}
    \caption{Column diagram of averaged accuracy percentages for NER tags
        (B-<tag> and I-<tag> are combined in PER, LOC and ORG)
    }\label{chart:tag-acc-ner}
\end{figure}

Despite this design flaw, we still get some notable observations from the
obtained \texttt{F1} scores, depicted in
Figure~\ref{chart:f1-by-batch-and-lang}.  First of all, we once again see that
the DyNet implementations are by far the most succesfull, having the highest
score in all but one experiment. This is again attributed to its 2 layer
implementation of the bidirectional LSTM network. We also see the same pattern
as for POS, where the PyTorch implementation has the second best results and
TensorFlow is falling severely behind.

We also see, that the \texttt{Bi-LSTM-CRF} models in PyTorch and TensorFlow
outperforms their \texttt{Bi-LSTM} counterparts at almost every batch size and
language (the notable exceptions being cases for japanase and norwegian, where
the scores are extraordinarily low. See Section~\ref{sec:experiments-ner-data}
for more). This is interesting, as it confirms the expectation that the CRF
layer is able to significantly improve the performance in performing NER tasks
(~\ref{}).

We still don't see a consistent improvement on the DyNet implementations from
the CRF layer. This again indicate, that the addition of a CRF
layer to a 2 layer \texttt{Bi-LSTM} model is of negligible effect for these
batch sizes. However, we might see a small tendency for the larger batch sizes
to benefit from the CRF, as the DyNet \texttt{Bi-LSTM-CRF} model highest score
in arabic, danish, russian and urdu.

\begin{figure}[h!]
    \includegraphics[width=\textwidth]{f1-no-crf}
    \includegraphics[width=\textwidth]{f1-with-crf}
    \caption{
    }\label{chart:f1-by-batch-and-lang}
\end{figure}

For completeness sake, we also include the data for our results on general
accuracy on the NER data in Table~\ref{table:acc-total-ner}. This was the actual
score our models trained to optimize and are as such relevant. 

Overall they align pretty well with the pattern found in the rest of the data,
but they are expectetly higher. The best results are generally either achieved
by the lowest batch size or by the \texttt{Bi-LSTM-CRF} and always by the DyNet
implementation. The best results reaches pretty impressive heights with
97.3\% for russian and 98.5\% for urdu. However since these accuracy scores can
get pretty high by just being really good at identifying non-entities, the
accuracy may not reflect the practical success of the models.

\begin{table}[h!]
    \centering
    \begin{tabular}{c l c c c|c c c}
        \toprule
        \multirow{2}{*}{\bfseries Language} &
        \multirow{2}{*}{\bfseries Batch size} &
        \multicolumn{3}{c}{\bfseries Bi-LSTM} &
        \multicolumn{3}{c}{\bfseries Bi-LSTM-CRF} \\
        \cmidrule(lr){3-8}
        && DyNet & PyTorch & Tensorflow & DyNet & PyTorch & Tensorflow \\

        \cmidrule(lr){1-8}
        \multirow{3}{*}{\bfseries ar}
        &  1 & 
        \underline{\textbf{88.4}} & \underline{87.4} & \underline{85.4} &
        88.3 & 84.1 & 81.6 \\
        &  8 & 
        \underline{\textbf{88.6}} & \underline{85.5} & \underline{81.1} &
        88.0 & 85.3 & 76.1 \\
        & 32 & 
        \underline{\textbf{88.9}} & 80.9 & \underline{77.0} &
        87.5 & \underline{84.8} & 71.5 \\

        \cmidrule(lr){1-8}
        \multirow{3}{*}{\bfseries da}
        &  1 &
        91.3 & \underline{90.5} & \underline{87.6} &
        \underline{\textbf{91.5}} & 89.8 & 81.6 \\
        &  8 &
        91.2 & 88.0 & \underline{85.3} &
        \underline{\textbf{91.2}} & \underline{90.4} & 75.8 \\
        & 32 &
        \underline{\textbf{91.1}} & 86.2 & 67.1 &
        91.1 & \underline{88.8} & \underline{74.2} \\

        \cmidrule(lr){1-8}
        \multirow{3}{*}{\bfseries hi}
        &  1 &
        \underline{\textbf{75.6}} & \underline{74.0} & \underline{72.0} &
        75.5 & 70.0 & 62.1 \\
        &  8 &
        74.6 & 71.5 & \underline{62.2} &
        \underline{\textbf{74.9}} & \underline{72.9} & 60.7 \\
        & 32 &
        74.3 & 64.8 & 34.4 &
        \underline{\textbf{74.5}} & \underline{70.1} & \underline{53.8} \\

        \cmidrule(lr){1-8}
        \multirow{3}{*}{\bfseries ja}
        &  1 &
        \underline{\textbf{83.7}} & \underline{80.0} & \underline{80.1} &
        81.4 & 52.2 & 72.8 \\
        &  8 &
        \underline{\textbf{82.4}} & \underline{80.5} & 77.8 &
        81.5 & 73.7 & \underline{79.4} \\
        & 32 &
        \underline{\textbf{82.1}} & \underline{79.4} & 77.1 &
        81.3 & 73.8 & \underline{81.0} \\

        \cmidrule(lr){1-8}
        \multirow{3}{*}{\bfseries no}
        &  1 &
        94.4 & \underline{86.4} & 87.2 &
        \underline{\textbf{94.4}} & 74.4 & \underline{87.3} \\
        &  8 &
        95.1 & 77.5 & \underline{85.3} &
        \underline{\textbf{95.3}} & \underline{86.4} & 82.0 \\
        & 32 &
        \underline{\textbf{94.2}} & \underline{85.5} & \underline{84.5} &
        93.1 & 81.7 & 82.1 \\

        \cmidrule(lr){1-8}
        \multirow{3}{*}{\bfseries ru}
        &  1 &
        \underline{\textbf{97.3}} & \underline{96.5} & 90.8 &
        97.1 & 96.1 & \underline{93.6} \\
        &  8 &
        \underline{\textbf{97.2}} & 96.0 & 85.6 &
        97.0 & \underline{97.1} & \underline{93.3} \\
        & 32 &
        96.9 & 94.1 & 72.3 &
        \underline{\textbf{97.1}} & \underline{97.0} & \underline{91.4} \\

        \cmidrule(lr){1-8}
        \multirow{3}{*}{\bfseries ur}
        &  1 &
        \underline{\textbf{98.5}} & 97.8 & 98.1 &
        98.0 & \underline{98.3} & \underline{98.1} \\
        &  8 &
        98.1 & 97.7 & 96.7 &
        \underline{\textbf{98.1}} & \underline{98.4} & \underline{97.2} \\
        & 32 &
        98.2 & 96.1 & 95.4 &
        \underline{\textbf{98.3}} & \underline{97.3} & \underline{96.9} \\
        \bottomrule
    \end{tabular}
    \caption{Accuracy in percentage for NER experiments by language and batch
        size. Bold: highest accuracy for batch size plus language. Underline:
        highest accuracy for framework between \texttt{Bi-LSTM} and
        \texttt{Bi-LSTM-CRF}.
    }\label{table:acc-total-ner}
\end{table}



\pagebreak
