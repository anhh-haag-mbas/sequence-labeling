
\section{Introduction}

\subsection{Project background}


The project was conducted as a bachelor thesis as part of the Software
Development programme at the IT University of Copenhagen. The project period ran
from February 2nd to May 15th 2019. Originally, the attached supervisor was
Zeljko Agic, who was involved in deciding the scope and the main problems of the
project. Early in the process, however, he was substituted with Leon Derczynski
of the Department of Computer Science at the IT University of Copenhagen, who
supervised the project group until submission and examination.

The code, report and results of the project are available at the projects GitHub
repository at \url{https://github.itu.dk/USS-NLterPrise/sequence-labelling}.


\subsection{Project description}

Some of the most fundamental machine learning applications in natural language
processing (NLP) are performing sequence labelling on sentences by processing
one word at a time and attribute it some label. A lot of ingenuity has been put
into designing machine learning models that specializes in tasks like these and
most popular machine learning framework supports these models.

In this project, we aim to examine and compare the performance of two of the
most successful such models, namely a bidirectional Long-Short Term Memory
network (LSTM) and a bidirectional LSTM with an added Conditional Random Field
(CRF) layer. We want to test how they manage part-of-speech tagging (POS) and
named entity recognition (NER), two traditional sequence labelling tasks, and
how different configurations of the training time and batch size affect this. We
will implement the models in three different machine learning frameworks so as
to compare the frameworks for ease of use, speed and performance.

Furthermore, we will run our tests across a number of languages that differ in
word ordering and language family and see if we can discern any pattern in what
kind of languages the models may be better or worse at processing.


\subsection{Project plan}

The project was initiated in February and planned to consist of 4 different
phases

In the first, the project group would get familiarized with concepts and theory
of machine learning, natural language processing and their respective
frameworks. As the project group mostly had no prior knowledge of any of these
concepts, this part was planned to stretch until mid March.

In the second phase, the experiments were created and the specific
configurations were decided based on the studies in phase 1 and discussion with
the project supervisor. Also in this phase, each project group member worked
with his respective framework to implement the models and prepare the
experiments.

The third phase was set to begin in the middle of April but got a bit delayed
due to longer time spent in phase 2 than expected. In this phase, the
experiments were run, analyzed and, for some cases, run again to fix certain
issues. During the experiments running, report writing was initiated.

The fourth phase ran from the end of April to the submission deadline on May
15th, and only consisted in writing the report and finishing the analysis of the
results.


\pagebreak
