

\subsection{Data}

This section will describe the process of finding and preparing the relevant
language data for the project. This includes considerations about which
languages to work with, what data sources was used, how the data was
preprocessed, etc.


\subsubsection{Choosing languages}

About choosing languages based on word ordering (and availability of good
datasets)


\subsubsection{Data gathering}

About our data sources (eg.\ Universal Dependencies), labelled datasets, size of
datasets and stuff like dat.


\subsubsection{Preprocessing}

Making all datasets the same size, and possibly something about how the data is
preprocessed in our programs (eg.\ adding padding)


[DRAFT]

The first part of the data gathering process was to find pre-existing annotated data for our two tasks, part-of-speech (POS), and named entitiy recognition (NER).
We wanted pre-existing datasets since it would be {too complicated (Jeg ved der eksisterer et bedre ord her men jeg kan ikke huske hvad det er lige nu)} to annotated data for the languages ourselves.
For the POS task we used datasets from~\ref{}{UniversalDependencies.org} which has a broad selection of languages with multiple datasets in each.
For the NER task we used auto-annotated wikipedia data from~\ref{}{http://nlp.cs.rpi.edu/wikiann/} created for the paper~\ref{}{Cross-lingual Name Tagging and Linking for 282 Languages}.
As the title suggests, this dataset contains annotated data for 282 languages in varying sizes.

The datasets from~\ref{}{UniversalDependencies.org} are all in the CoNNL-U format, but are created from different types of sources. 
The source types are given as tags such as news, legal, blog, wiki, etc.
The datasets from~\ref{}{http://nlp.cs.rpi.edu/wikiann/} are all in BIO format, and since they were created based on wikipedia we didn't have to take varying sources into considerations.

We are interested in whether word ordering has an impact on the accuracy of our models so our primary criteria for languages were their word ordering, ie. the relative ordering of verbs, subjects, and objects (eg. verb-object-subject).
Word ordering were decided based on~\ref{}{https://wals.info/chapter/81}, we picked languages with one dominant word-ordering, ignoring languages such as german which has alternating word orderings. 

For word orderings with enough languages to choose from we picked two languages from the same langauge family and one language from a different family.
Language families were similarly based on~\ref{}{https://wals.info/languoid}, we split languages based on both their family and genus, meaning a language such as german (indo-european, germanic) is considered different from italian (indo-european, romance).

We wanted the same languages for both tasks and also required that there was a pre-trained polyglot word embedding for the langauge. 
This left us with around 60 languages to choose from. 
We also wanted to train or models on the same number of sentences in each language to avoid any language performing much better solely because of more training. 
Due to the nature of the languages each sentence might contain a greater or lesser amount of words in each sentence, we didn't do anything to equalize this, since the word count wouldn't affect the number of updates when training.
There is an argument to be made that since the language would also be tested on longer sentences, and we might assume it is harder to correctly label longer sentences than shorter ones, the accuracy might be lower for these languages than others. 

The cutoff we made on the number of sentences was 5000, where 4000 was used for training and the last 1000 was split evenly between validation and testing.
Based on this we had data for three languages for the two most common word orderings subject-object-verb and subject-verb-object.
For the less common verb-subject-object word ordering we didn't have sufficient data for more than a single language, and for even rarer word orderings we didn't have any data.

The treebanks we selected from~\ref{}{UniversalDependencies.org} were all made from news, where some used additional sources such as non-fiction and spoken.
A list of the languages as well as the word-ordering, language family, treebank, and tags is given below.

* Arabic,   VSO, Afro-Asiatic, Semitic, PADT, news
* Hindi,    SOV, IE, Indic,             HDTB, news
* Urdu,     SOV, IE, Indic,             UDTB, news
* Japanese, SOV, Japanese,              GSD, blog, news
* Danish,   SVO, IE, Germanic,          DDT, fiction, news, non-fiction, spoken
* Norwegian,SVO, IE, Germanic,          Bokmaal, blog, news, non-fiction
* Russian,  SVO, IE, Slavic,            SynTagRus, fiction, news, non-fiction

The distribution of tokens, labels, etc. is given in appendix.

For the training, validation and test-sets the data was simply split, as following. 
First 4000 sentences for training, next 500 for validation, and next 500 for testing.
Since the datasets from~\ref{}{UniversalDependencies.org} comes pre-split, these were concatenated before splitting.
Some datasets, such as the Norwegian dataset, contains contractions for some words in the~\ref{}{UniversalDependencies.org} as well as the individual words.
Since the contration is usually unlabelled in the datasets these were simply removed from the data for easy of parsing.

As each framework handles the data in different ways, more detail as to how the data was further pre-processed is given in their respective chapters.
