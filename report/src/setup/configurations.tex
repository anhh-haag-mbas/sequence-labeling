
\subsection{Configurations}

There are a total of 3780 different combinations of configurations based on the
7 parameters we decided to work with. 

A short list of the parameters are given below, and are explained more in detail
in the subsequent chapters.

\newcommand{\conf}[2]{%
  \StrCut{#2}{&}\csA\csB
  \begin{tabular}{#1}
    \textbf{\csA} & \csB \\
  \end{tabular}
}

\conf{llll}{Frameworks & DyNet & PyTorch & TensorFlow}

\conf{llllllll}{Languages & Arabic & Danish & Hindi & Japanese & Norwegian & Russian & Urdu}

\conf{lll}{Tasks & Part-of-speech & Named Entity Recognition}

\conf{lll}{Models & Bi-LSTM & Bi-LSTM-CRF}

\conf{llllll}{Seeds & 322233 & 521403 & 421213 & 5123 & 613321}

\conf{llll}{Mini-batch-size & 1 & 8 & 32}

\conf{llll}{Epochs & 1 & 5 & 50 using early stopping with a patience of 3}

\subsubsection{Languages}

The rationale behind the languages picked is a combination of word ordering,
language family and genus, availability of data, and the origin of the data.

The word ordering is the relative ordering of verbs, subjects, and objects (eg.\
verb-object-subject or VOS). We used (\cite{wals-81}) to determine the word
ordering of languages and picked languages with one dominant word ordering,
ignoring languages such as German which has alternating word orderings.

Language families are groupings of languages based on some common ancestral
language. Languages from the same family often share similarities such as word
orderings and some words. 

For word orderings with many languages to choose from, we picked two languages
from the same language family, and one language from a different family.
Language families were similarly based on (\cite{wals-languoid}). We split
languages based on both their family and genus, meaning a language such as
German (indo-european, germanic) is considered different from Italian
(indo-european, romance).

Sadly due to limited availability of pre-existing annotated datasets, we
couldn't find data on more than 3 different word orderings. For the more common
word orderings, subject-object-verb and subject-verb-object, there was a descent
range of languages to choose from. But for the less common word orderings we
could only find a single verb-subject-object language with enough data. There
was no data for the remaining languages. 

The 7 languages we picked, along with their word ordering, language family and
genus is given table~\ref{table:langfam}.

\begin{table}[!ht]
  \centering
  \begin{tabular}{llll}
    \textbf{Language} & \textbf{Word ordering} & \textbf{Family} & \textbf{Genus} \\
    \midrule
    Arabic    & VSO & Afro-Asiatic  & Semitic \\
    Hindi     & SOV & Indo-European & Indic \\
    Urdu      & SOV & Indo-European & Indic \\
    Japanese  & SOV & Japanese      & \\
    Danish    & SVO & Indo-European & Germanic \\
    Norwegian & SVO & Indo-European & Germanic \\
    Russian   & SVO & Indo-European & Slavic \\
  \end{tabular}
  \caption{Overview over picked languages, their word ordering, language
  family and genus.
  }\label{table:langfam}
\end{table}

The concern of availability of data and the origin is elaborated on
in~\ref{sec:experiments-pos-data}.


\subsubsection{Model implementations}

All experiments have been conducted across three popular and open source machine
learning frameworks, two dynamic and one static. Each member of the project
group have had responsibility for the implementation of the models in one of the
frameworks. This section will give a descriptive introduction to each framework,
how it works and the implementation details relevant for this project. For a
discussion and evaluation of each framework, refer to
Section~\ref{subsec:frameworks}.

\subsubsection*{A note on computational graphs}

When describing neural networks the goto abstraction is that of neurons which
are linked to each other and arranged in layers. A neuron receives the output of
each neuron from the previous layer, computes its own value, and sends it to the
next layer alongside all the other neurons in its layer. Since it would be too
computationally expensive to calculate the result of each neuron individually
the neural network abstraction is discarded in favour of a matrix and vector
based representation which is closer to how the results are calculated in the
computer. The concept of layers however remains, so a simple layer could be
represented as the calculation $\sigma(W \times \bm{x} + B)$, where $\sigma$ would be
the activation function sigmoid
(\cite[Chapter~6.5.1]{goodfellow2016deep},~\cite[Chapter~5.1]{goldberg2017neural}).

When neural networks are implemented in frameworks the layer abstraction is also
ignored in favor of so called computational graphs. In a computational graph
each node in the graph represents a single mathematical operation, or an input
value. As such the simple layer from before would be split into three distinct
nodes connected in succession. The values for $W$ and $\bm{x}$ would be input into a
matrix multiplication node. The result of this node would be the input alongside
$B$ to a vector addition node, which would be the only input value to the
sigmoid node. 

Computational graphs as such don't provide much of an advantage over the layer
abstraction. Conceptionally we are still doing the same mathematical operations
on the input and the performance of this model is not better than the layer based
representation, since we are still taking advantage of the same performance
gains from working with vectors and matricies. Instead, the reason behind the use
of computational graphs is because it simplifies calculating the gradients of
the network through the backpropagation algorithm. 

The backpropagation algorithm uses the chain rule of derivatives to calculate
the derivative of input values wherever they may be in the computational graph.
Since the chain rules is used for computing the derivative of arbitrarily many
functions, and computational graphs represents each function by its own node,
the graph abstraction simplifies calculating derivatives since each node only
needs to know the rules for computing the derivate of itself.
 

\subsubsection*{DyNet}\label{sec:setup-implementations-dynet}

DyNet is an open source dynamic neural network framework developed by Garnegie
Mellon University and many others (\cite{dynet-git}). The framework being
dynamic means the computational graphs are recreated for each input. This is
particularly useful for natural language processing where sentences don't have a
fixed length so the unique structure of the graph of each sentences can be made
without having to add padding to match the longest sentence in the dataset. A
downside of dynamic frameworks is that they are generally slower than their
static counterparts. The creators of DyNet, though, claim that it performs
comparable or faster than similar static declaration frameworks (\cite{dynet}).   

DyNet further allows for what they call auto-batching which means that sentences
don't have to be manually batched when training on batches of
sentences (\cite{autobatching}). In other frameworks, dynamic or not, users have
to add a dimension to their input which represents the batches. In DyNet this
can be done by implicitly creating a graph for each input in the batch and
aggregate the losses, creating a combined computational graph, which can be
optimized to run parts in parallel. The DyNet implementation aggregated using
the sum of the losses, however this was not possible for the other frameworks
due to what we suspect was overflow errors, which wasn't experienced in DyNet. 

A consequence of the framework being dynamic is that the structure of the model
is mixed with the processing and training on input. The model parameters are
defined by themselves, but since the input is an integral part of the
computation graph, the structure is defined by the mathematical operations
performed on the specific input values instead of a more general structure which
works on any input. This only has an impact on the structure of the source code,
and in extension how easy it is to understand it. The input should be processed
in the same way as it would be in a static framework, assuming the same
structure has been defined. This also means that instead of thinking of the
model as layers it is often better to think about the underlying operations of
the layers. The exception is the LSTM layer where much of the
implementation detail has been hidden behind a concise interface.

For training the \code{SimpleSGDTrainer} was used. For the embedding layer, the
polyglot embedding was converted to could be used to initialize a
\code{LookupParameter}. The Bi-LSTM was implemented using the
\code{LSTMBuilder}, and the \code{BiRNNBuilder}. Dropout could be implemented
using the \code{dropout} method. Early stopping was implemented by saving the
currently best seen model each time the model was improved upon. This creates
some overhead, but nothing noticable. If no improvements were seen for 3 epochs,
the model was loaded and training terminated.

DyNet doesn't have a CRF layer implemented as part of their library so we based
our implementation on the work of~\cite{plank2016multilingual}. The loss
function for the CRF model is not the negative log likelihood. The reference
implementation uses the instance score of the forward algorithm minus the
gold score of the sentence. The influence this may have had one the results are
unclear.

Due to miscomunication the DyNet model used two Bi-LSTM layers. This has been
shown to give better accuracy at the cost of longer training times. We decided
to prefer the shorter training over the better accuracy, but didn't make this
clear. As a result, we would expect DyNet to perform better in the experiments. 


\subsubsection*{PyTorch}\label{sec:setup-implementations-pytorch}

PyTorch is an open source, dynamic deep learning toolkit build on top of Torch
and developed by the Facebook research team (\cite{yegulalp2017facebook}).
PyTorch provides a \code{tensor} object, that is nearly identical to NumPys
\code{ndarray} but has two important advantages:

\begin{itemize}
    \item It allows for seamless computation through the GPU instead of the CPU
    \item It records the computational graph on the fly, allowing for easy
        backpropagation
\end{itemize}

As opposed to DyNet, PyTorch does not have any auto-batching feature. It does
support mini-batches through its API and most of its models and functions accept
a \code{batch\_first} argument where relevant, which indicate whether the first
dimension of the input corresponds to the batch size or not.

We create the input mini-batches for the model as a preprocessing step, by
splitting the data into lists containing a number of data elements (ie.\
sentences) as specified by the \code{batch\_size} parameter. Every sentence is
padded with a special padding token \code{''<PAD>''} so it has the same length as
the longest sentence in the batch. In addition, the batch is sorted by sentence
length and in the process, a list of original sentence lengths as well as a
masking matrix to mask out padding values are created. This is a requirement for
several of the PyTorch modules.

By mistake, the batching process was implemented in a way, so that some data is
lost when the data is not evenly divisible by the batch size. For example, had
the batch size been 3 and the number of data elements been 20, then the last
$20\mod3 = 2$ sentences would be omitted. Luckily, since all our training data
consists of excactly 4000 sentences, it is divisible by all 3 batch sizes we use
(1, 8 and 32). When running our experiments, however, we use 500 sentences for
testing, meaning that $500\mod8 = 4$ and $500\mod32 = 20$ sentences were omitted
for the experiments with batch size 8 and 32 respectively. This is a minor note
to keep in mind.

As for the actual implementation of the two models in PyTorch, as much
pre-existing code within the framework as possible was used. This includes the
\code{Embedding}, \code{Dropout}, \code{LSTM} and \code{Linear} layer (all
provided through the \code{torch.nn} module), aswell as the
\code{CrossEntropyLoss} function for calculating the loss on the
\texttt{Bi-LSTM} model. The general structure of the model, the training
function and the evaluation function were created from tutorials, examples and
existing applications (see~\cite{pytorch2017lstm},~\cite{yang2018ncrf},
and \url{https://github.com/pytorch/examples}).

The CRF layer was implemented using the existing PyPi package
\texttt{pytorch-crf} (v.0.7.2) by Kemal Kurniawan under the MIT
license (available at \url{https://github.com/kmkurn/pytorch-crf}).

To utilize early stopping with patience during training, we did not explore the
built-in functionalities that do exist in PyTorch. Instead, this was implemented
manually, by measuring the models accuracy in predicting tags on the validation
data set after each epoch. A counter keeps track of how many epochs in a row the
model hasn't seen any improvement in accuracy and if this counter gets to 3 the
training stops.

One caveat, though. The model that is returned after training is not the one
that achieved the highest accuracy. Rather, it is the one that saw the third
consequtive drop in accuracy which means that it is more likely a sub-optimal
model. This is another note to keep in mind when analyzing the results.


\subsubsection*{TensorFlow}\label{sec:tf_conf}

TensorFlow is an open source static deep learning framework developed primarily
by Google. Keras is a high-level API for frameworks like TensorFlow, CNTK and
Theano.

A significant effort has been spent to align TensorFlow with Keras, so much so
that it is sometimes hard to tell if you are dealing with pure TensorFlow
functionality or a Keras API\@. Further thoughts on this can be found in
Section~\ref{sec:discuss_tensorflow}.

As such our implementation is based on Keras with TensorFlow as its
backend.

While the API allows manual control over how batching is done, it defaults to
just handling batching automatically. As such little to no extra effort was
spent implementing batching.

Early stopping is done using an \code{EarlyStopping} callback when training the
model.

Because the framework is static padding the input data is required, however
since we are using recurrent neural networks we can deal with dynamically sized
input. This is done by setting \code{maskZero=True} in the RNN layer, which
then prevents padding from affecting the quality of the training.

However, the TensorFlow results are run with padding set to a non-zero value.
This was not intentional and greatly affects the accuracy and comparability of
the TensorFlow results.

Whenever possible built-in compononts of Keras or Tensorflow was used, this
includes \code{Dense}, \code{Dropout}, \code{Embedding} and \code{LSTM} layers.
The CRF implementation is based upon a CRF layer by
Hironsan (available at \url{https://github.com/Hironsan/keras-crf-layer}).

\subsubsection{Experimental setup}

For all languages, frameworks, and tasks we ran experiments on three different
mini-batch sizes, three epoch settings, and with five seeds. Additional
consideration were also made in regards to the training data to make for as fair
comparisons as possible.

The three mini-batch sizes were motivated by (\cite{reimers2017reporting}) who
reported worse accuracy for larger sizes of mini-batches with a sweet spot
between 8 and 32. Similar low values are used by (\cite{huang2015bidirectional}).
While this emperically seems to work, it is a bit counter to the theory. Ideally
we would train on the whole training set to get the most accurate loss values,
which would in turn allow us to make the optimal update to the model to reduce
the loss. We don't have to use the whole training set and can do fine with an
estimate of the true loss by using mini-batches, however we would still expect
bigger mini-batch sizes to improve the estimate and result in better, but slower
convergence (\cite[Chapter~8.1.3]{goodfellow2016deep}).

The fact that bigger mini-batch sizes leads to worse results, suggests that a
better estimate is not necessarily better for training. While a better estimate
of the true loss results in a more optimal update to lower the loss, the greater
variance in the loss estimate can also work as a regularization technique which
results in better generalization (\cite[pp.~279]{goodfellow2016deep}). The lower
mini-batch sizes can then be explained as a way to avoid the model from getting
stuck in local minima.

The 3 epochs is to see how well the models perform based on very limited
training, on some reasonable amount of training, and the upper limits of the
models for the tasks. The expectation is that a single epoch of training gives
very bad performance on the tasks. 5 epochs is the baseline where we expect
that none of the models have begun overfitting, while still able to produce some
reasonable predictive power. The last epoch setting is early stopping with a
patience of 3, and a max of 50 epochs. This means that the model continues
training untill it has made 3 updates, which didn't improve the accuracy on
the validation set. Early testing suggests that models stopped improving before
the max, so we expect this setting to reasonable represents the best of what the
models can achieve. It is possible that with a lot more training, or different
hyperparameters, that the models can get an even better performance than our
early stopping allows.

The different seeds are used to get a better idea of the performance of the
models than a single run would give. This approach is based on the work
of (\cite{reimers2017reporting}), and (\cite{yang2018design}) who demonstrated how a
single result comparison between two models is insufficient when randomness is
involved. If one model performs better than average on a run, and another model
performs worse than average, this might suggest that the model which performed
better is a better model. But if more runs are used it is possible to compare
the average, minimum and maximum accuracy of models, along with the standard
deviation to give a better perspective on the relative performance of the
models. This helps mitigate situations where we mistakenly conclude that one
model is better than another.

To make comparisons between languages more fair, we limited the data on the
number of sentences for training, testing, and validation. The cutoff made was
5000 sentences, where 4000 is used for training and the last 1000 was split
evenly between validation and testing. The numbers picked were based on
availability of data with a preference for more sentences. From the datasets,
the first 4000 sentences were used for training, next 500 for validation, and
next 500 for testing. Any remaining sentences in the data were discarded.

Due to the nature of the languages each training set might contain a differing
number of words in each sentence. We didn't do anything to equalize this since
the word count wouldn't affect the number of updates when training. There is a
risk that since the language would also be tested on longer sentences, and we
may assume it is harder to correctly label longer sentences than shorter ones,
the accuracy might be lower for these languages than others. The languages with
longer sentences in their test set is, however, most likely also the languages
with longer sentences in their training set. This hopefully evens out the
mistakes.

The datasets found are created from different types of sources, eg.\ news, blog
posts, reviews, etc. We wanted the datasets for the different languages to be
created from similar sources. It was infeasible to find datasets for both tasks
which were created from similar sources, but within each task, the data is to
some degree made from the same kind of source. The details of the data (ie.\ the
tag distributions, the number of tokens, etc.) were not considered before
examining the results of the experiments, and are therefore described and
reviewed in Section~\ref{sec:experiments}.

To make model implementation easier, the datasets from the different tasks were
standardized such that there was no need for task specific code.

The size of our training sets are relatively small. The Penn Wall Streets
journal treebank (\cite{wsj}), often used for the part of speech task, contains
over 1.6 million words, where our datasets, depending on the language, contain
around than 100.000 words split over training, testing, and validation files. As
more data generally improves the accuracy of the models we would expect our
models to do worse than models trained on a larger dataset, assuming similar
network structure. The size is however not too small to learn a good
representation.~\cite{plank2016multilingual} found, when comparing the accuracy
of different types of taggers, that when training on as little as 1000 sentences
their model could get over 90\% accuracy on a wide range of languages.

We used the polyglot package to load the word embeddings. Specifically the
Embeddings class from the polyglot mappings library. There are a lot of
different polyglot models available, some are task specific for tasks we used
even. We used the embeddings2 model for all models, in their respective
language. 

\pagebreak
