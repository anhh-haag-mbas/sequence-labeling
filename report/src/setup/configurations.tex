
\subsection{Configurations}

There are a total of 3780 different combinations of configurations based on the
7 parameters we decided to work with. 

A short list of the parameters are given below, and are explained more in detail
in the subsequent chapters.

%TODO: Make nice table or something
Frameworks: Dynet, PyTorch, TensorFlow

Languages: Arabic, Danish, Hindi, Japanese, Norwegian, Russian, Urdu

Tasks: Part-of-speech, Named Entity Recognition

Models: Bi-LSTM, Bi-LSTM-CRF

Seeds: 322233, 521403, 421213, 5123, 613321

Mini-batch-size: 1, 8, 32

Epochs: 1, 5, 50 using early stopping patience 3


\subsubsection{Languages}

The rationale behind the languages picked is a combination of word ordering,
language family and genus, availability of data, and the origin of the data.

The word ordering is the relative ordering of verbs, subjects, and objects (eg.\
verb-object-subject or VOS). We used~\ref{}{https://wals.info/chapter/81} to
determine the word ordering of languages and picked languages with one dominant
word ordering, ignoring languages such as German which has alternating word
orderings.

Language families are groupings of languages based on some common ancestral
language. Languages from the same family often share similarities such as word
orderings and some words. 

For word orderings with many languages to choose from, we picked two languages
from the same language family, and one language from a different family.
Language families were similarly based on~\ref{}{https://wals.info/languoid}. We
split languages based on both their family and genus, meaning a language such as
German (indo-european, germanic) is considered different from Italian
(indo-european, romance).

Sadly due to limited availability of pre-existing annotated datasets, we
couldn't find data on more than 3 different word orderings. For the more common
word orderings, subject-object-verb and subject-verb-object, there was a descent
range of languages to choose from. But for the less common word orderings we
could only find a single verb-subject-object language with enough data. There
was no data for the remaining languages. 

The 7 languages we picked, along with their word ordering, language family and
genus is given in~\ref{} below.

% TODO: Make nice table
* Arabic,   VSO, Afro-Asiatic, Semitic 

* Hindi,    SOV, Indo-European, Indic

* Urdu,     SOV, Indo-European, Indic

* Japanese, SOV, Japanese

* Danish,   SVO, Indo-European, Germanic

* Norwegian,SVO, Indo-European, Germanic

* Russian,  SVO, Indo-European, Slavic

The concern of availability of data and the origin is elaporated on
in~\ref{}{3.1}


\subsubsection{Model implementations}

All experiments have been conducted across three popular and open source machine
learning frameworks, two dynamic and one static. Each member of the project
group have had responsibility for the implementation of the models in one of the
frameworks. This section will briefly describe the experience and caveates with
working with each of the frameworks.

This subsection will give a descriptive introduction to each framework, how it
works and the implementation details relevant for this project. For a discussion
and evaluation of each framework, refer to Section~\ref{subsec:frameworks}.

\subsubsection*{A note on computational graphs}

% TODO: Move to relevant section
When describing neural networks the goto abstraction is that of neurons which
are linked to each other and arranged in layers. A neuron receives the output of
each neuron from the previous layer, computes its own value, and sends it to the
next layer alongside all the other neurons in its layer. Since it would be too
computationally expensive to calculate the result of each neuron individually
the neural network abstraction is discarded in favour of a matrix and vector
based representation which is closer to how the results are calculated in the
computer. The concept of layers however remains, so a simple layer could be
represented as the calculation $\sigma(W \cdot X + B)$, where $\sigma$ would be
the activation function sidmoid. 

\ref{Deep learning 6.5.1, NNM for NLP ???}

When neural networks are implemented in frameworks the layer abstraction is also
ignored in favor of so called computational graphs. In a computational graph
each node in the graph represents a single mathematical operation, or an input
value. As such the simple layer from before would be split into three distinct
nodes connected in succession. The values for $W$ and $X$ would be input into a
matrix multiplication node. The result of this node would be the input alongside
$B$ to a vector addition node, which would be the only input value to the
sigmoid node. 

Computational graphs as such don't provide much of an advantage over the layer
abstraction. Conceptionally we are still doing the same mathematical operations
on the input the performance of this model is not better than the layer based
representation, since we are still taking advantage of the same performance
gains from doing working with vectors and matricies. The reason behind the use
of computational graphs is because it simplifies calculating the gradients of
the network through the backpropagation algorithm. 

The backpropagation algorithm uses the chain rule of derivatives to calculate
the derivative of input values wherever they may be in the computational graph.
Since the chain rules is used for computing the derivative of arbitrarily many
functions, and computational graphs represents each function by its own node,
the graph abstraction simplifies calculating derivatives since each node only
needs to know the rules for computing the derivate of itself.
 

\subsubsection*{DyNet}

% Some general stuff about dynamic frameworks which might work better elsewhere?
% Also is the comparison of difference between dynamic and static frameworks
% even correct?
DyNet is an open source dynamic neural network framework, where the
computational graphs are recreated for each input. This is particularly useful
for natural language processing where sentences don't have a fixed length so the
unique structure of each sentence's graph can be made without having to add
padding to match the longest sentence in the dataset. A downside of dynamic
frameworks is that they are generally slower than their static counterparts,
the creators of DyNet though mention that it performs comparable or faster than
similar static declaration frameworks~\ref{DyNet report}.   

DyNet further allows for what they call auto-batching which means that sentences
don't have to be manually batched when training on batches of
sentences~\ref{On-the-fly Operation Batching in Dynamic Computation Graphs}. In
other frameworks, dynamic or not, users have to add a dimension to their input
which represents the batches.

% Perhaps the single core thing was just CRF
DyNet doesn't support multi-threading in their python implementation as of the
time of this report, so all the experiments were run on a single core. This had
some unforseen consequences on the runtime which is further elaporated in the
discussion Section~\ref{discussion}.

A consequence of the framework being dynamic is that the structure of the model
is mixed with the processing and training on input. The model parameters are
defined by themselves, but since the input is an integral part of the
computation graph, the structure is defined by the mathematical operations
performed on the specific input values instead of a more general structure which
works on any input. This only has an impact on the structure of the source code,
and in extension how easy it is to understand it. The input should be processed
in the same way as it would be in a static framework, assuming the same
structure has been defined. This also means that instead of thinking of the
model as layers it is often better to think about the underlying operations of
the layers. The exception is the LSTM layer where much of the
implementation detail has been hidden behind a concise interface.

DyNet doesn't have a CRF layer implemented as part of their library so we based
our implementation on the work of~\ref{Barbara}, which in turn based parts of
theirs on~\ref{Someone else, see their source code}. 

Due to miscomunication the DyNet model used two Bi-LSTM layers, this gives has
been shown to give better accuracy at the cost of longer training times. We
decided to prefer the shorter training over the better accuracy, but didn't make
this clear. As a result, we would expect DyNet to perform better in the
experiments.


% Wait! How the f does DyNet gain a speed increase when using mini-batching if
% it doesn't utilize multiple cores!!???!?!

\subsubsection*{PyTorch}

PyTorch is an open source, dynamic deep learning toolkit build on top of Torch
and developed by the Facebook research team (~\ref{}
\url{https://www.infoworld.com/article/3159120/facebook-brings-gpu-powered-machine-learning-to-python.html}).
PyTorch provides a \code{tensor} object, that is nearly identical to NumPys
\code{ndarray} but has two important advantages:

\begin{itemize}
    \item It allows for seamless computation through the GPU instead of the CPU
    \item It records the computational graph on the fly, allowing for easy
        backpropagation
\end{itemize}

As opposed to DyNet, PyTorch does not have any auto-batching feature. It does
support mini-batches through its API and most of its models and functions accept
a \code{batch\_first} argument where relevant, which indicate whether the first
dimension of the input corresponds to the batch size or not.

However, batching is rather poorly described by the documenation of the
framework, and there seem to be only sparse official description of how
mini-batches are expected to be arranged\footnote{\url{https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e}}.
As an example, working with sequential data of variable length (such as
sentences), each input element has to padded so as to make the lengths uniform.
This may be common knowledge to an experienced ML researcher, but neither the
PyTorch documentation or official tutorials (at least not under the topic of
NLP) touch on this aspect.

Furthermore, to handle these batches of inputs of variable lengths, some modules
require the client to pass a masking tensor as argument (ie.\ a matrix of ones
and zeroes) to mask out padded values. Most ill-explained, however, is using
mini-batches together with the \code{LSTM} layer provided by the framework
itself. Here, it is necessary to pass the mini-batch of padded input sequences
(ordered by original sequence length in descending order) through
\code{pack\_padded\_sequence} function provided by the \code{torch.nn.utils.rnn}
module together with a list of the original lengths of each sequence. This
transforms the input so it can be handled properly by the \code{LSTM} layer, but
afterwards it has to be unpacked again using \code{pad\_packed\_sequence}. Not
only is this a rather intricate API, it is also only sparsely documented
considering how normal an operation training on mini-batches is.

As for the actual implementation of the two models in PyTorch, as much
pre-existing code within the framework as possible was used. This includes the
\code{Embedding}, \code{Dropout}, \code{LSTM} and \code{Linear} layer (all
provided through the \code{torch.nn} module), aswell as the
\code{CrossEntropyLoss} function for calculating the loss on the
\texttt{Bi-LSTM} model. The general structure of the model, the training
function and the evaluation function were created from tutorials, examples and
existing applications.\footnote{\url{https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html}}\footnote{\url{https://github.com/jiesutd/NCRFpp}}\footnote{\url{https://github.com/pytorch/examples}}

The CRF layer was implemented using the existing PyPi package
\texttt{pytorch-crf} (v.0.7.2) by Kemal Kurniawan under the MIT
license.\footnote{\url{https://github.com/kmkurn/pytorch-crf}}


\subsubsection*{TensorFlow}

Haha, good luck



\subsubsection{Experimental setup}

For all languages, frameworks, and tasks we ran experiments on three different
mini-batch sizes, three epoch settings, and with five seeds. Additional
consideration were also made in regards to the training data to make for as fair
comparisons as possible.

The three mini-batch sizes were motivated by~\ref{Reymer and gurevych} who
reported worse accuracy for larger sizes of mini-batches with a sweet spot
between 8 and 32. As will be expanded on in the discussion, this seems counter
intuitive to the general idea behind the use of mini-batches. % TODO: discuss

The three epochs is to see how well the models perform based on very limited
training, on some reasonable amount of training, and the upper limits of the
models for the tasks. The expectation is that a single epoch of training gives
very bad performance on the tasks. Five epochs is the baseline where we expect
that none of the models have begun overfitting, while still able to produce some
reasonable predictive power. The last epoch setting is early stopping with a
patience of three, and a max of 50 epochs. This means that the model continues
training intill it has made three updates, which didn't improve the accuracy on
the validation set. Early testing suggests that models stopped improving before
the max, so this setting reasonable represents the best of what the models can
achieve. It is possible that with a lot more training, or different
hyperparameters, that the models can get an even better performance than our
early stopping allows.

The different seeds are used to get a better idea of the performance of the
models than a single run would give. This approach is based on the work
of~\ref{Reimers+Gurevych, misconceptions in nsl yang zhang} who demonstrated how
a single result comparison between two models is insufficient when randomness is
involved. If one model performs better than average on a run, and another model
performs worse than average, this might suggest that the model which performed
better is a better model. But if more runs are used it is possible to compare
the average, minimum and maximum accuracy of models, along with the standard
deviation to give a better perspective on the relative performance of the
models. This helps mitigate situations where we mistakenly conclude that one
model is better than another.

To make comparisons between languages more fair, we limited the data on the
number of sentences for training, testing, and validation. The cutoff made was
5000 sentences, where 4000 is used for training and the last 1000 was split
evenly between validation and testing. The numbers picked were based on
availability of data with a preference for more sentences. From the datasets,
the first 4000 sentences were used for training, next 500 for validation, and
next 500 for testing. Any remaining sentences in the data were discarded.

Due to the nature of the languages each training set might contain a differing
number of words in each sentence, we didn't do anything to equalize this since
the word count wouldn't affect the number of updates when training. There is a
risk that since the language would also be tested on longer sentences, and we
may assume it is harder to correctly label longer sentences than shorter ones,
the accuracy might be lower for these languages than others. The languages with
longer sentences in their test set however is most likely also the languages
with longer sentences in their training set this hopefully evens out the
mistakes.

The datasets found are created from different types of sources, eg.\ news, blog
posts, reviews, etc. We wanted the datasets for the different languages to be
created from similar sources. It was infeasible to find datasets for both tasks
which were created from similar sources, but within each task, the data is to
some degree made from the same kind of source, the details are included in their
respective chapters.

There was an issue in the japanese dataset where we ignored a lot of the tokens used.
The size of our training sets are relatively small. The Penn Wall Streets
journal treebank \ref{https://catalog.ldc.upenn.edu/LDC95T7} often used for the
part of speech task, contains over 1.6 million words, where our datasets,
depending on the language, contain around than 100.000 words split over
training, testing, and validation files. As more data generally improves the
accuracy of the models we would expect our models to do worse than models
trained on a larger dataset, assuming similar network structure. The size is
however not too small to learn a good representation.
\ref{https://www.aclweb.org/anthology/P16-2067} found, when comparing the
accuracy of different types of taggers, that when training on as little as 1000
sentences their model could get over 90\% accuracy on a wide range of languages.

% TODO: Extra notes

The datasets from the different tasks were standardizes such that there was no
need for task specific code. 

We used the polyglot package to load the word embeddings. Specifically the
Embeddings class from polyglot mappings library. 
There are a lot of different polyglot embedding 


\pagebreak
