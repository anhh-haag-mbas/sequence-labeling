
\subsection{Configurations}

There are a total of 3780 different combinations of configurations based on the
7 parameters we decided to work with. 

A short list of the parameters are given below, and are explained more in detail
in the subsequent chapters.

%TODO: Make nice table or something
Frameworks: Dynet, PyTorch, TensorFlow

Languages: Arabic, Danish, Hindi, Japanese, Norwegian, Russian, Urdu

Tasks: Part-of-speech, Named Enitity Recognition

Models: Bi-LSTM, Bi-LSTM-CRF

Seeds: 322233, 521403, 421213, 5123, 613321

Mini-batch-size: 1, 8, 32

Epochs: 1, 5, 50 using early stopping patience 3

%The first part of the data gathering process was to find pre-existing annotated
%data for our two tasks, part-of-speech (POS), and named entitiy recognition
%(NER). We wanted pre-existing datasets since it would be {too complicated (Jeg
%ved der eksisterer et bedre ord her men jeg kan ikke huske hvad det er lige nu)}
%to annotated data for the languages ourselves. For the POS task we used datasets
%from~\ref{}{UniversalDependencies.org} which has a broad selection of languages
%with multiple datasets in each. For the NER task we used auto-annotated
%wikipedia data from~\ref{}{http://nlp.cs.rpi.edu/wikiann/} created for the
%paper~\ref{}{Cross-lingual Name Tagging and Linking for 282 Languages}. As the
%title suggests, this dataset contains annotated data for 282 languages in
%varying sizes.

%The datasets from~\ref{}{UniversalDependencies.org} are all in the CoNNL-U
%format, but are created from different types of sources. The source types are
%given as tags such as news, legal, blog, wiki, etc.\ The datasets
%from~\ref{}{http://nlp.cs.rpi.edu/wikiann/} are all in BIO format, and since
%they were created based on wikipedia we didn't have to take varying sources into
%considerations.

%The cutoff we made on the number of sentences was 5000, where 4000 was used for
%training and the last 1000 was split evenly between validation and testing.
%Based on this we had data for three languages for the two most common word
%orderings subject-object-verb and subject-verb-object. For the less common
%verb-subject-object word ordering we didn't have sufficient data for more than a
%single language, and for even rarer word orderings we didn't have any data.
%
%The treebanks we selected from~\ref{}{UniversalDependencies.org} were all made
%from news, where some used additional sources such as non-fiction and spoken. A
%list of the languages as well as the word-ordering, language family, treebank,
%and tags is given below.
%
%* Arabic,   VSO, Afro-Asiatic, Semitic, PADT, news
%* Hindi,    SOV, IE, Indic,             HDTB, news
%* Urdu,     SOV, IE, Indic,             UDTB, news
%* Japanese, SOV, Japanese,              GSD, blog, news
%* Danish,   SVO, IE, Germanic,          DDT, fiction, news, non-fiction, spoken
%* Norwegian,SVO, IE, Germanic,          Bokmaal, blog, news, non-fiction
%* Russian,  SVO, IE, Slavic,            SynTagRus, fiction, news, non-fiction
%
%The distribution of tokens, labels, etc.\ is given in appendix.
%
%For the training, validation and test-sets the data was simply split, as
%following. First 4000 sentences for training, next 500 for validation, and next
%500 for testing. Since the datasets from~\ref{}{UniversalDependencies.org} comes
%pre-split, these were concatenated before splitting. Some datasets, such as the
%Norwegian dataset, contains contractions for some words in
%the~\ref{}{UniversalDependencies.org} as well as the individual words. Since the
%contration is usually unlabelled in the datasets these were simply removed from
%the data for easy of parsing.
%
%As each framework handles the data in different ways, more detail as to how the
%data was further pre-processed is given in their respective chapters.


\subsubsection{Languages}

The rationale behind the languages picked is a combination of word ordering,
language family and genus, availability of data, and the origin of the data.

The word ordering is the relative ordering of verbs, subjects, and objects (eg.
verb-object-subject or alternatively VOS). We used
~\ref{}{https://wals.info/chapter/81} to determine the word ordering of
languages and picked languages with one dominant word ordering, ignoring
languages such as German which has alternating word orderings. 

Language families are groupings of languages based on some common ancestral
language. Languages from the same family often share similarities such as word
orderings and individual words. For word orderings with many languages to choose
from, we picked two languages from the same langauge family, and one language
from a different family. Language families were similarly based
on~\ref{}{https://wals.info/languoid}. We split languages based on both their
family and genus, meaning a language such as German (indo-european, germanic) is
considered different from Italian (indo-european, romance).

Sadly due to limited availability of data, we couldn't find data on more than 3
different word orderings, subject-object-verb, subject-verb-object, and a single
language for verb-subject-object. 

The 7 languages we picked, along with their word ordering, language family and
genus is given in \ref{} below.

% TODO: Make nice table
* Arabic,   VSO, Afro-Asiatic, Semitic 
* Hindi,    SOV, Indo-European, Indic
* Urdu,     SOV, Indo-European, Indic
* Japanese, SOV, Japanese
* Danish,   SVO, Indo-European, Germanic
* Norwegian,SVO, Indo-European, Germanic
* Russian,  SVO, Indo-European, Slavic

The concern of availability of data and the origin is elaporated on in
\ref{}{Another chapter}.


We wanted the same languages for both tasks and also required that there was a
pre-trained polyglot word embedding for the langauge. This left us with around
60 languages to choose from. We also wanted to train or models on the same
number of sentences in each language to avoid any language performing much
better solely because of more training. Due to the nature of the languages each
sentence might contain a greater or lesser amount of words in each sentence, we
didn't do anything to equalize this, since the word count wouldn't affect the
number of updates when training. There is an argument to be made that since the
language would also be tested on longer sentences, and we might assume it is
harder to correctly label longer sentences than shorter ones, the accuracy might
be lower for these languages than others. 



\subsubsection{Frameworks}

All experiments have been conducted across three popular and open source machine
learning frameworks, two dynamic and one static. Each member of the project
group have had responsibility for the implementation of the models in one of the
frameworks. This section will briefly describe the experience and caveates with
working with each of the frameworks.

This subsection will give a descriptive introduction to each framework, how it
works and the implementation details relevant for this project. For a discussion
and evaluation of each framework, refer to Section~\ref{subsec:frameworks}.


\subsubsection*{DyNet}

DyNet is an open source dynamic neural network framework, where the
computational graphs is recreated for each input. This is particularly useful
for natural language processing where sentences don't have a fixed length so the
unique structure of each sentence's graph can be made without having to add
padding to match the longest sentence in the dataset.

DyNet further allows for what they call auto-batching which means that sentences
don't have to be manually batched when training on batches of sentences
~\ref{On-the-fly Operation Batching in Dynamic Computation Graphs}. In other
frameworks, dynamic or not, users have to add the a dimension to their input
which represents the batches.

DyNet doesn't support multi-threading in their python implementation yet, so we
all the experiments were run on a single core. This had some unforseen
consequences on the runnning time which is further elaporated in the discussion
chapter ~\ref{discussion}.

% Wait! How the f does DyNet gain a speed increase when using mini-batching if
% it doesn't utilize multiple cores!!???!?!

\subsubsection*{PyTorch}

This was the dynamic framework that couldn't handle different sized batches.


\subsubsection*{TensorFlow}

Haha, good luck



\subsubsection{Experimental setup}


\pagebreak
