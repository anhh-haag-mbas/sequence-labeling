
\subsection{Configurations}

There are a total of 3780 different combinations of configurations based on the
7 parameters we decided to work with. 

A short list of the parameters are given below, and are explained more in detail
in the subsequent chapters.

%TODO: Make nice table or something
Frameworks: Dynet, PyTorch, TensorFlow

Languages: Arabic, Danish, Hindi, Japanese, Norwegian, Russian, Urdu

Tasks: Part-of-speech, Named Enitity Recognition

Models: Bi-LSTM, Bi-LSTM-CRF

Seeds: 322233, 521403, 421213, 5123, 613321

Mini-batch-size: 1, 8, 32

Epochs: 1, 5, 50 using early stopping patience 3


%The treebanks we selected from~\ref{}{UniversalDependencies.org} were all made
%from news, where some used additional sources such as non-fiction and spoken. A
%list of the languages as well as the word-ordering, language family, treebank,
%and tags is given below.
%
%* Arabic,   VSO, Afro-Asiatic, Semitic, PADT, news
%* Hindi,    SOV, IE, Indic,             HDTB, news
%* Urdu,     SOV, IE, Indic,             UDTB, news
%* Japanese, SOV, Japanese,              GSD, blog, news
%* Danish,   SVO, IE, Germanic,          DDT, fiction, news, non-fiction, spoken
%* Norwegian,SVO, IE, Germanic,          Bokmaal, blog, news, non-fiction
%* Russian,  SVO, IE, Slavic,            SynTagRus, fiction, news, non-fiction
%
%The distribution of tokens, labels, etc.\ is given in appendix.
%
%For the training, validation and test-sets the data was simply split, as
%following. First 4000 sentences for training, next 500 for validation, and next
%500 for testing. Since the datasets from~\ref{}{UniversalDependencies.org} comes
%pre-split, these were concatenated before splitting. Some datasets, such as the
%Norwegian dataset, contains contractions for some words in
%the~\ref{}{UniversalDependencies.org} as well as the individual words. Since the
%contration is usually unlabelled in the datasets these were simply removed from
%the data for easy of parsing.
%
%As each framework handles the data in different ways, more detail as to how the
%data was further pre-processed is given in their respective chapters.


\subsubsection{Languages}

The rationale behind the languages picked is a combination of word ordering,
language family and genus, availability of data, and the origin of the data.

The word ordering is the relative ordering of verbs, subjects, and objects (eg.
verb-object-subject or alternatively VOS). We used
~\ref{}{https://wals.info/chapter/81} to determine the word ordering of
languages and picked languages with one dominant word ordering, ignoring
languages such as German which has alternating word orderings. 

Language families are groupings of languages based on some common ancestral
language. Languages from the same family often share similarities such as word
orderings and some words. 

For word orderings with many languages to choose from, we picked two languages
from the same langauge family, and one language from a different family.
Language families were similarly based on~\ref{}{https://wals.info/languoid}. We
split languages based on both their family and genus, meaning a language such as
German (indo-european, germanic) is considered different from Italian
(indo-european, romance).

Sadly due to limited availability of pre-existing annotated datasets, we
couldn't find data on more than 3 different word orderings. For the more common
word orderings, subject-object-verb and subject-verb-object, there was a descent
range of languages to choose from. But for the less common word orderings we
could only find a single verb-subject-object language with enough data. There
was no data for the remaining languages. 

The 7 languages we picked, along with their word ordering, language family and
genus is given in \ref{} below.

% TODO: Make nice table
* Arabic,   VSO, Afro-Asiatic, Semitic 

* Hindi,    SOV, Indo-European, Indic

* Urdu,     SOV, Indo-European, Indic

* Japanese, SOV, Japanese

* Danish,   SVO, Indo-European, Germanic

* Norwegian,SVO, Indo-European, Germanic

* Russian,  SVO, Indo-European, Slavic

The concern of availability of data and the origin is elaporated on in
\ref{}{Another chapter}.


\subsubsection{Frameworks}

All experiments have been conducted across three popular and open source machine
learning frameworks, two dynamic and one static. Each member of the project
group have had responsibility for the implementation of the models in one of the
frameworks. This section will briefly describe the experience and caveates with
working with each of the frameworks.

This subsection will give a descriptive introduction to each framework, how it
works and the implementation details relevant for this project. For a discussion
and evaluation of each framework, refer to Section~\ref{subsec:frameworks}.


\subsubsection*{DyNet}

DyNet is an open source dynamic neural network framework, where the
computational graphs is recreated for each input. This is particularly useful
for natural language processing where sentences don't have a fixed length so the
unique structure of each sentence's graph can be made without having to add
padding to match the longest sentence in the dataset. A downside of dynamic
frameworks is that they are generally slower than their static counterparts,
the creators of DyNet though mention that it performs comparable or faster than
similar static declaration frameworks ~\ref{DyNet report}.   

DyNet further allows for what they call auto-batching which means that sentences
don't have to be manually batched when training on batches of sentences
~\ref{On-the-fly Operation Batching in Dynamic Computation Graphs}. In other
frameworks, dynamic or not, users have to add a dimension to their input which
represents the batches.

DyNet doesn't support multi-threading in their python implementation yet, so all
the experiments were run on a single core. This had some unforseen consequences
on the runtime which is further elaporated in the discussion chapter
~\ref{discussion}.

A consequence of the framework being dynamic is that the structure of the model
is mixed with the processing and training on input. The model parameters are
defined by themselves, but since the input is an integral part of the graph, the
structure is defined by the mathematical operations performed on the specific
input values instead of a more general structure which works on any input. This
only has an impact on the structure of the source code, and in extension how
easy it is to understand it. The input is processed in the same way as it would
be in a static framework. This also means that instead of thinking of the model
as layers it is often better to think about the underlying operations of the
layers. The exception is the LSTM layer where much of the implementation detail
has been hidden behind a concise interface.

DyNet doesn't have a CRF layer implemented as part of their library so we based
our implementation on the work of ~\ref{Barbara}, which in turn based parts of
theirs on ~\ref{Someone else, see their source code}. 

% Wait! How the f does DyNet gain a speed increase when using mini-batching if
% it doesn't utilize multiple cores!!???!?!

\subsubsection*{PyTorch}

This was the dynamic framework that couldn't handle different sized batches.


\subsubsection*{TensorFlow}

Haha, good luck



\subsubsection{Experimental setup}

For all languages, frameworks, and tasks we ran experiments on three different
mini-batch sizes, three epoch settings, and with five seeds. Additional
consideration were also made in regards to the training data to make for as fair
comparisons as possible.

The three mini-batch sizes were motivated by ~\ref{Reymer and gurevych} who
reported worse accuracy for larger sizes of mini-batches with a sweet spot
between 8 and 32. As will be expanded on in the discussion, this seems counter
intuitive to the general idea behind the use of mini-batches. % TODO: discuss

The three epochs is to see how well the models perform based on very limited
training, on some reasonable amount of training, and the upper limits of the
models for the tasks. The expectation is that a single epoch of training gives
very bad performance on the tasks. Five epochs is the baseline where we expect
that none of the models have begun overfitting, while still able to produce some
reasonable predictive power. The last epoch setting is early stopping with a
patience of three, and a max of 50 epochs. This means that the model continues
training intill it has made three updates which didn't improve the accuracy on
some validation set. Early testing suggests that models stopped improving before
the max, so this setting reasonable represents the best of what the models can
achieve. It is possible that with a lot more training, or different
hyperparameters, that the models can get an even better performance than our
early stopping allows.

The different seeds are used to get a better idea of the performance of the
models than a single run would give. This approach is based on the work of
~\ref{Reimers+Gurevych, misconceptions in nsl yang zhang} who demonstrated how a
single result comparison between two models is insufficient when randomness is
involved. If one model performs better than average on a run, and another model
performs worse than average, this might suggest that the model which performed
better is a better model. But if more runs are used it is possible to compare
the average, minimum and maximum accuracy of models, along with the standard
deviation to give a better perspective on the relative performance of the
models. This helps mitigate situations where we mistakenly conclude that one
model is better than another.

%The first part of the data gathering process was to find pre-existing annotated
%data for our two tasks, part-of-speech (POS), and named entitiy recognition
%(NER). We wanted pre-existing datasets since it would be infeasible
%to annotated data for the languages ourselves. For the POS task we used datasets
%from~\ref{}{UniversalDependencies.org} which has a broad selection of languages
%with multiple datasets in each. For the NER task we used auto-annotated
%wikipedia data from~\ref{}{http://nlp.cs.rpi.edu/wikiann/} created for the
%paper~\ref{}{Cross-lingual Name Tagging and Linking for 282 Languages}. As the
%title suggests, this dataset contains annotated data for 282 languages in
%varying sizes.
%
%The datasets from~\ref{}{UniversalDependencies.org} are all in the CoNNL-U
%format, but are created from different types of sources. The source types are
%given as tags such as news, legal, blog, wiki, etc.\ The datasets
%from~\ref{}{http://nlp.cs.rpi.edu/wikiann/} are all in BIO format, and since
%they were created based on wikipedia we didn't have to take varying sources into
%considerations.

To make comparisons between languages more fair, we limited the data on the
number of sentences for training, testing, and validation. The cutoff made was
5000 sentences, where 4000 is used for training and the last 1000 was split
evenly between validation and testing. The numbers picked were based on
availability of data with a preference for more sentences. 

Due to the nature of the languages each training set might contain a differing
number of words in each sentence, we didn't do anything to equalize this since
the word count wouldn't affect the number of updates when training. There is a
risk that since the language would also be tested on longer sentences, and we
might assume it is harder to correctly label longer sentences than shorter ones,
the accuracy might be lower for these languages than others. The languages with
longer sentences in their test set however is most likely also the languages
with longer sentences in their training set this hopefully evens out the
mistakes.

The datasets found are created from different types of sources, eg. news, blog
posts, reviews, etc. We wanted the datasets for the different languages to be
created from similar sources. It was infeasible to find datasets for both tasks
which were created from similar sources, but within each task, the data is to
some degree made from the same kind of source, the details are included in their
respective chapters.


\pagebreak
