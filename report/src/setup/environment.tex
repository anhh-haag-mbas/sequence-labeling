\subsection{Machine environment}

The experiments were executed across several AWS EC2 machines, as they offer
quite powerful virtual machines.

\subsubsection{Detailed description of machine environemnt}

The experiments ran concurrently in their own process with multithreading
turned on for the frameworks that supported it. Most of the experiments ran on
an instance of type \texttt{c5.9xlarge}, some ran on \texttt{c5.18xlarge}.
Each machine had a 32 GiB magnetic harddisk attached as its primary storage.
The key difference was the vCPU count, as the memory difference had no
impact on execution.
[ref: https://aws.amazon.com/ec2/instance-types/]

For dynet 34 processes were executed concurrently on a \texttt{c5.9xlarge} EC2
machine.
For tensorflow 5 processes were executed concurrently on a \texttt{c5.9xlarge}
EC2 machine.
Pytorch initally ran 2 processes concurrently with multithreading enabled on a
\texttt{c5.9xlarge} EC2 machine, but was switched to 61 processes with
multithreading disabled on a \texttt{c5.18xlarge} EC2 machine midway through the
execution of the experiments.

\subsubsection{Differences between AWS and development environment}

Notable differences between execution in the development environment and on the
EC2 servers were noticed and are summarised by the following:

Pytorch grinded to a halt on the 36 core EC2 machine. Switching to a 72 core EC2
machine without disabling multithreading only exacerbated the problem. It turns
out that pytorch's multithreading does not support a high amount of cores [ref].

Tensorflow had no noticable differences between the development environment and
the EC2 machines.

Some dynet configurations took several days to complete on the EC2 machine,
whereas it completed in a matter of minutes in the local development
environment. The exact cause is not known to us. 

\subsubsection{Comparability of speed results}

As such none of the speed results are truly comparable. This a result of
several factors.
One being that 2 different EC2 instance types was used, where one was faster
than the other, or in pytorch's case slower.
Another is the fact that each experiment was run concurrently with others. This
would have only had a minor impact, if multithreading had not been enabled for
some of the frameworks.
Lastly dynet behaved unexpectedly with regards to execution time, compared to the
development environment.

Ideally each experiment would be run one at a time, or with multithreading
disabled, on identical EC2 machines. Seeing as that was not the case, no
conclusions can be made with regards to the speed of the frameworks, only hints
at where further experimentation with tighter control might be warranted.

% Here we could add something about master/slave pattern og maybe how utilising
% AWS Batch would have been "oplagt"
