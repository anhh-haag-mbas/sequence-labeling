
\subsection{Models}

This section will cover the main theoretical aspects of the project and give a
general description of the mathematical principles behind the machine learning
models used in this project. It will also describe the different configurations
for our experiments and models, aswell as it will motivate the use of Polyglot
embeddings.


\subsubsection{General structure of the models}

We used two different models to run our experiments. The first one is a standard
bidirectional LSTM network, \texttt{Bi-LSTM}, and the second one is a
\texttt{Bi-LSTM} with an added CRF layer, \texttt{Bi-LSTM-CRF}. Both models has
the same fundamental structure, which consist of:

\begin{itemize}
    \item An embedding layer with a fixed embedding dimension size
    \item A dropout layer with a dropout rate of $0.5$
    \item Two LSTM layers, one for the forward pass and one for the backward
        pass
    \item Another dropout layer with the same dropout rate
    \item A simple linear layer mapping the LSTM output scores back into tag
        space
\end{itemize}

In the \texttt{Bi-LSTM} model, the output of the final linear layer is run
through a softmax activation function. In the \texttt{Bi-LSTM-CRF} model, the
output is instead treated as emission scores that is passed as input to the CRF
layer, which then compute the most likely sequence of tags.



\subsubsection{Embeddings}

We use a pretrained embedding since they have been shown to provide a
significant increase in accuracy for sequence labeling tasks (~\ref{}).
We picked pretrained Polyglot embeddings since there exists models in a lot of
different languages, and Polyglot seems to outperform its competitors like
FastText (~\ref{} to FastText + comparison), even though FastText's embeddings are
significantly larger than Polyglot's.


\subsubsection{Bi-LSTM}

Long Short-Term Memory (LSTM) networks are a particular type of Recurrent Neural
Networks (RNN), that was originally proposed by (~\ref{} Hochreiter 199X) as a way of
dealing with defiencies of normal RNNs. For machine learning tasks concerned
with sequential data it is more often than not necessary to let information
about an input at a timestep $t$ get carried over to the processing of an input
at timestep $t+1$. This is the motivation of RNN (see Figure~\ref{}).

However, the standard RNN suffers from the problems of exponential decay or blow
up of error signals that are backpropagated through the network --- a situation
that only gets worse as the time dependencies increase (~\ref{} Hochreiter,
Graves). To remedy this, LSTM networks are used instead.

An LSTM network is compposed of carefully designed memory cells containing
multiple gates, that determines what information to remember, what to forget,
what to do with the input and the output of the network. In addition, a main
state, $c$, of the network cell is maintained and used to compute a hidden
state, $h$, which is passed on to be used at the next timestep.

The gates and states are calculated in the following manner:\footnote{Note that
    these calculations are done by the respective frameworks we have been
working with and are not something, we have implemented ourselves}

\begin{align*}
    & i_{t} = \sigma(W_{xi}x_{t} + W_{hi}h_{t-1} + W_{ci}c_{t-1} + b_{i})    \\
    & f_{t} = \sigma(W_{xf}x_{t} + W_{hf}h_{t-1} + W_{cf}c_{t-1} + b_{f})    \\
    & o_{t} = \sigma(W_{xo}x_{t} + W_{ho}h_{t-1} + W_{co}c_{t-1} + b_{o})    \\ \\
    & c_{t} = f_{t}c_{t-1} + i_{t}\tanh(W_{xc}x_{t} + W_{hc}h_{t-1} + b_{c}) \\
    & h_{t} = o_{t}\tanh(c_{t})
\end{align*}

where $i_{t}$, $f_{t}$ and $o_{t}$ are the input gate, the forget gate and the
output gate respectively at time $t$, $c$ is the cell state and $h$ is the
hidden state. The weight matrices $W$ maps from either an input $x$ of size $m$
to an internal state or gate of size $n$ (ie. $W_{xf} \in \mathbb{R}^{m*n}$ is the
input-forget gate matrix), or between two internal components (ie. $W_{hi} \in
\mathbb{R}^{n*n}$ is the hidden-input gate matrix). (~\ref{} Huang et al. 2015)

The activation function $\sigma$ is the sigmoid function, which squishes the
values of the gate matrices into the interval $[0,1]$. This means, that the
gates determine which and how much information is passed through and stored in
the memory cell. For example, the forget gate $f_{t}$ at timestep $t$ is 
multiplied with the previous cell state $c_{t-1}$ to determine what to remember
and what to forget when $c_{t}$ is computed. A value of $1$ in the gate matrix
means `keep all' where a $0$ means `forget all'.
\footnote{This particular forget mechanism was not
    introduced by Hochreiter in his original LSTM paper and according to
    \url{https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html}
    `led the original [\ldots] model to have trouble with simple tasks involving
    long sequences'. This solution was introduced by (~\ref{} Gers et al. 2000)
and is now standard in LSTM implementations.}

Updating the cell state also entails adding the element wise product of the
input gate $i_{t}$ and \textit{the candidate write} given by $\tanh(W_{xc}x_{t}
+ W_{hc}h_{t-1} + b_{c})$. Since the hidden state $h_{t} = o_{t}\tanh(c_{t})$
the candidate write at timestep $t$ consists of not only the weighted current
input $W_{xc}x_{t}$ but also the weighted result of passing the previous cell
state through the previous output gate $W_{hc}h_{t-1}$ where $h_{t-1} =
o_{t-1}\tanh(c_{t-1})$.

In other words, the memory cell at timestep $t-1$ produces a reading of its own
cell state which it passes on as a hidden state to the memory cell at timestep
$t$ to be used to compute the candidate write for $c_{t}$. For further
discussion of this somewhat reversed read/write procedure, refer to (~\ref{}
\url{https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html}).

The intricate design of the LSTM memory cell allows for time dependencies
spanning very long sequences through careful regulation of the cell state.
Applying a separate LSTM network layer that processes the same sequence of data
but in reversed order, the model becomes bidirectional (see Figure~\ref{}).
However, at each timestep $t$ there will be two memory cells: one for the
forward pass and one for the backward pass. The first will depend on all $x_{i}
\in \{x_{0}, \ldots, x_{t-1}\}$ where the second will depend on all $x_{j} \in
\{x_{t+1}, \ldots, x_{T}\}$. This requires that the entire sequence of data is
provided before an output can be produced, but at the same time it enables the
output to be computed taking both past and future dependencies into
consideration.


% \url{http://colah.github.io/posts/2015-08-Understanding-LSTMs/}

% \url{https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html#written-memories-the-intuition-behind-lstms}



\subsubsection{CRF}

When performing sequence labelling as we do in part-of-speech tagging and named
entity recognition, we want to predict a sequence of output tags $\bm{y} \in
\mathbb{R}^{K}$, where $K$ is the sequence length, given a sequence of input
features $\bm{X} \in \mathbb{R}^{K \times C}$ where $C$ is the number of
possible classes each tag can take.

The Conditional Random Field model (CRF) provides a method for computing the
probability of an output sequence $\bm{y}$ given an input sequence $\bm{X}$
under the assumption, that there exists conditional dependencies for
transitioning between any two $y_{k}$ and $y_{k+1}$ where $k \leq K$. Knowing these
transitioning scores, which will be the learnable parameters of the CRF model,
we can compute the conditional probability of specific sequence of tags given an
input sequence of feature vectors $p(\bm{y}|\bm{X})$.

The input sequence is often called the emission scores and that is what we will
use here onwards. The emission scores in our model comes from the feature
representation generated by the Bi-LSTM network after they have been passed
through the linear layer and mapped to tag space. 

For a regular classification problem, we could now compute $p(\bm{y}|\bm{X})$ by
taking the product of the probability of all $y_{k} \in \bm{y}$ and dividing it
by the normalization factor $Z(\bm{X}) = \prod_{k=1}^{K} Z(\bm{x}_{k})$ (also
known as the \textit{partition function}:

\begin{align*}
p(\bm{y}|\bm{X}) & = \prod_{k=1}^{K} \frac{\exp( W(\bm{x}_{k}, y_{k}) )} 
                                            {Z(\bm{x}_{k})} \\
                 & = \frac{\exp( \sum_{k=1}^{K} W(\bm{x}_k, y_{k}) )}
                                            {Z(\bm{X})}
\end{align*}

where $W(y_{k}, \bm{x}_{k})$ is the emission scores generated by the LSTM
network, here represented with a weight matrix $W$ to be consistent with general
notation (~\ref{}
\url{https://towardsdatascience.com/implementing-a-linear-chain-conditional-random-field-crf-in-pytorch-16b0b9c4b4ea}).

For a conditional random field model however, we want to include the
dependency between label $y_{k-1}$ and $y_{k}$. This can be done by multiplying
with the probability of $y_{k}$ given $y_{k-1}$, ie. $p(y_{k}|y_{k-1})$. For
this, a transition matrix $T \in \mathbb{R}^{C \times C}$ is introduced, which 
is the trainable parameter of the CRF model:

\begin{equation*}
p(\bm{y}|\bm{X}) = \frac{\exp( \sum_{k=1}^{K} W(\bm{x}_k, y_{k}) +
                    \sum_{k=1}^{K-1} T(y_{k+1}, y_{k}) )}{Z(\bm{X})}
\end{equation*}

The challenge from here is to calculate the partition function $Z(\bm{X})$ as
this has to sum over all possible values of $y_{k}$ for all $1 \leq K$. This is
infeasable if implemented in a naive manner, but the whole crux of CRF is the
\textbf{forward} (or \textbf{backward}) algorithm, which enables the computation
to be done in time proportional to $O(CK^{2})$. A detailed description is out of
scope for this report, but is explained in great detail in (~\ref{} Sutton et
al. XXXX).

Performing inference using a CRF model (ie.\ predicting the most likely sequence
of tags $\bm{y^{*}} = \argmax_{y}p(\bm{y}|\bm{X})$) is done using the Viterbi
algorithm (~\ref{} something). This exploits the same recursive feature of the
model as the forward algorithm uncovers, and then stores backpointers to the
most likely tag $y^{*}$ at each time step $k$.

In short, our \texttt{Bi-LSTM-CRF} model is simply an extension of the
\texttt{Bi-LSTM} with an added CRF layer for modelling the conditional
dependencies between each successive tag in the sequence. 










