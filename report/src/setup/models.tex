
\subsection{Models}

This section will cover the main theoretical aspects of the project and give a
general description of the mathematical principles behind the machine learning
models used in this project. It will also describe the different configurations
for our experiments and models, aswell as it will motivate the use of Polyglot
embeddings.


\subsubsection{General structure of the models}

We used two different models to run our experiments. The first one is a standard
bidirectional LSTM network, \texttt{Bi-LSTM}, and the second one is a
\texttt{Bi-LSTM} with an added CRF layer, \texttt{Bi-LSTM-CRF}. Both models has
the same fundamental structure, which consist of:

\begin{itemize}
    \item An embedding layer with a fixed embedding dimension size
    \item A dropout layer with a dropout rate of $0.5$
    \item Two LSTM layers, one for the forward pass and one for the backward
        pass
    \item Another dropout layer with the same dropout rate
    \item A simple linear layer mapping the LSTM output scores back into tag
        space
\end{itemize}

In the \texttt{Bi-LSTM} model, the output of the final linear layer is run
through a softmax activation function. In the \texttt{Bi-LSTM-CRF} model, the
output is instead treated as emission scores that is passed as input to the CRF
layer, which then compute the most likely sequence of tags.



\subsubsection{Embeddings}

We use a pretrained embedding since they have been shown to provide a
significant increase in accuracy for sequence labeling tasks (~\ref{}).
We picked pretrained Polyglot embeddings since there exists models in a lot of
different languages, and Polyglot seems to outperform its competitors like
FastText (~\ref{} to FastText + comparison), even though FastText's embeddings are
significantly larget than Polyglot's.


\subsubsection{Bi-LSTM}

Long Short-Term Memory (LSTM) networks are a particular type of Recurrent Neural
Networks (RNN), that was originally proposed by (~\ref{} Hochreiter 199X) as a way of
dealing with defiencies of normal RNNs. For machine learning tasks concerned
with sequential data it is more often than not necessary to let information
about an input at a timestep $t$ get carried over to the processing of an input
at timestep $t+1$. This is the motivation of RNN (see Figure~\ref{}).

However, the standard RNN suffers from the problems of exponential decay or blow
up of error signals that are backpropagated through the network --- a situation
that only gets worse as the time dependencies increase (~\ref{} Hochreiter,
Graves). To remedy this, LSTM networks are used instead.

An LSTM network is compposed of carefully designed memory cells containing
multiple gates, that determines what information to remember, what to forget,
what to do with the input and the output of the network. In addition, a main
state, $c$, of the network cell is maintained and used to compute a hidden
state, $h$, which is passed on to be used at the next timestep.

The gates and states are calculated in the following manner:\footnote{Note that
    these calculations are done by the respective frameworks we have been
working with and are not something, we have implemented ourselves}

\begin{align*}
    & i_{t} = \sigma(W_{xi}x_{t} + W_{hi}h_{t-1} + W_{ci}c_{t-1} + b_{i})    \\
    & f_{t} = \sigma(W_{xf}x_{t} + W_{hf}h_{t-1} + W_{cf}c_{t-1} + b_{f})    \\
    & o_{t} = \sigma(W_{xo}x_{t} + W_{ho}h_{t-1} + W_{co}c_{t-1} + b_{o})    \\ \\
    & c_{t} = f_{t}c_{t-1} + i_{t}\tanh(W_{xc}x_{t} + W_{hc}h_{t-1} + b_{c}) \\
    & h_{t} = o_{t}\tanh(c_{t})
\end{align*}

where $i_{t}$, $f_{t}$ and $o_{t}$ are the input gate, the forget gate and the
output gate respectively at time $t$, $c$ is the cell state and $h$ is the
hidden state. The weight matrices $W$ maps from either an input $x$ of size $m$
to an internal state or gate of size $n$ (ie. $W_{xf} \in \mathbb{R}^{m*n}$ is the
input-forget gate matrix), or between to internal components (ie. $W_{hi} \in
\mathbb{R}^{n*n}$ is the hidden-input gate matrix). (~\ref{} Huang et al. 2015)

The activation function $\sigma$ is the sigmoid function, which squishes the
values of the gate matrices into the interval $[0,1]$. This means, that the
gates determine which and how much information is passed through and stored in
the memory cell. For example, the forget gate $f_{t}$ at timestep $t$ is 
multiplied with the previous cell state $c_{t-1}$ to determine what to remember
and what to forget when $c_{t}$ is computed. A value of $1$ in the gate matrix
means `keep all' where a $0$ means `forget all'.
\footnote{This particular forget mechanism was not
    introduced by Hochreiter in his original LSTM paper and according to
    \url{https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html}
    `led the original [\ldots] model to have trouble with simple tasks involving
    long sequences'. This solution was introduced by (~\ref{} Gers et al. 2000)
and is now standard in LSTM implementations.}

Updating the cell state also entails adding the element wise product of the
input gate $i_{t}$ and \textit{the candidate write} given by $\tanh(W_{xc}x_{t}
+ W_{hc}h_{t-1} + b_{c})$. Since the hidden state $h_{t} = o_{t}\tanh(c_{t})$
the candidate write at timestep $t$ consists of not only the weighted current
input $W_{xc}x_{t}$ but also the weighted result of passing the previous cell
state through the previous output gate $W_{hc}h_{t-1}$ where $h_{t-1} =
o_{t-1}\tanh(c_{t-1})$.

In other words, the memory cell at timestep $t-1$ produces a reading of its own
cell state which it passes on as a hidden state to the memory cell at timestep
$t$ to be used to compute the candidate write for $c_{t}$. For further
discussion of this somewhat reversed read/write procedure, refer to (~\ref{}
\url{https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html}).

The intricate design of the LSTM memory cell allows for time dependencies
spanning very long sequences through careful regulation of the cell state.
Applying a separate LSTM network layer that processes the same sequence of data
but in reversed order, the model becomes bidirectional (see Figure~\ref{}).
However, at each timestep $t$ there will be two memory cells: one for the
forward pass and one for the backward pass. The first will depend on all $x_{i}
\in \{x_{0}, \ldots, x_{t-1}\}$ where the second will depend on all $x_{j} \in
\{x_{t+1}, \ldots, x_{T}\}$. This requires that the entire sequence of data is
provided before an output can be produced, but at the same time it enables the
output to be computed taking both past and future dependencies into
consideration.


% \url{http://colah.github.io/posts/2015-08-Understanding-LSTMs/}

% \url{https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html#written-memories-the-intuition-behind-lstms}



\subsubsection{CRF}

This was not fun at all!


